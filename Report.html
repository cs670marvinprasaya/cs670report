<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Prasaya Acharya and Marvin Chan">
<meta name="dcterms.date" content="2025-02-01">

<title>CS670 Data Science Project: Sentiment and Rating Analysis of Book Reviews</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="Report_files/libs/clipboard/clipboard.min.js"></script>
<script src="Report_files/libs/quarto-html/quarto.js"></script>
<script src="Report_files/libs/quarto-html/popper.min.js"></script>
<script src="Report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Report_files/libs/quarto-html/anchor.min.js"></script>
<link href="Report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Report_files/libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Report_files/libs/bootstrap/bootstrap-3b187030e5f7148da5c50070da05c9b1.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a>
  <ul>
  <li><a href="#setuppreprocessing" id="toc-setuppreprocessing" class="nav-link" data-scroll-target="#setuppreprocessing">Setup/preprocessing:</a></li>
  </ul></li>
  <li><a href="#exploratory-analysis" id="toc-exploratory-analysis" class="nav-link" data-scroll-target="#exploratory-analysis">Exploratory Analysis</a>
  <ul>
  <li><a href="#distribution-of-ratings" id="toc-distribution-of-ratings" class="nav-link" data-scroll-target="#distribution-of-ratings">Distribution of Ratings</a></li>
  <li><a href="#rating-vs.-review-text-word-count" id="toc-rating-vs.-review-text-word-count" class="nav-link" data-scroll-target="#rating-vs.-review-text-word-count">Rating vs.&nbsp;Review Text Word Count</a></li>
  <li><a href="#average-rating-per-genre" id="toc-average-rating-per-genre" class="nav-link" data-scroll-target="#average-rating-per-genre">Average Rating per Genre</a></li>
  </ul></li>
  <li><a href="#sentiment-analysis" id="toc-sentiment-analysis" class="nav-link" data-scroll-target="#sentiment-analysis">Sentiment Analysis</a>
  <ul>
  <li><a href="#bag-of-words-and-tf-idf-with-logistic-regression" id="toc-bag-of-words-and-tf-idf-with-logistic-regression" class="nav-link" data-scroll-target="#bag-of-words-and-tf-idf-with-logistic-regression">Bag of Words and TF-IDF with Logistic Regression</a>
  <ul class="collapse">
  <li><a href="#bag-of-words-model" id="toc-bag-of-words-model" class="nav-link" data-scroll-target="#bag-of-words-model">Bag of Words Model</a></li>
  <li><a href="#tf-idf-model" id="toc-tf-idf-model" class="nav-link" data-scroll-target="#tf-idf-model">TF-IDF Model</a></li>
  </ul></li>
  <li><a href="#tf-idf-with-simple-neural-network" id="toc-tf-idf-with-simple-neural-network" class="nav-link" data-scroll-target="#tf-idf-with-simple-neural-network">TF-IDF with Simple Neural Network</a></li>
  <li><a href="#comparison-of-bow-tf-idf-and-nn-models" id="toc-comparison-of-bow-tf-idf-and-nn-models" class="nav-link" data-scroll-target="#comparison-of-bow-tf-idf-and-nn-models">Comparison of BoW, TF-IDF and NN Models</a></li>
  <li><a href="#distilbert-for-sentiment-analysis" id="toc-distilbert-for-sentiment-analysis" class="nav-link" data-scroll-target="#distilbert-for-sentiment-analysis">DistilBERT for Sentiment Analysis</a></li>
  </ul></li>
  <li><a href="#multiclass-classification" id="toc-multiclass-classification" class="nav-link" data-scroll-target="#multiclass-classification">Multiclass Classification</a>
  <ul>
  <li><a href="#bag-of-words-and-tf-idf-with-logistic-regression-for-rating-prediction" id="toc-bag-of-words-and-tf-idf-with-logistic-regression-for-rating-prediction" class="nav-link" data-scroll-target="#bag-of-words-and-tf-idf-with-logistic-regression-for-rating-prediction">Bag of Words and TF-IDF with Logistic Regression for Rating Prediction</a>
  <ul class="collapse">
  <li><a href="#bag-of-words-for-rating-prediction" id="toc-bag-of-words-for-rating-prediction" class="nav-link" data-scroll-target="#bag-of-words-for-rating-prediction">Bag of Words for Rating Prediction</a></li>
  <li><a href="#tf-idf-for-rating-prediction" id="toc-tf-idf-for-rating-prediction" class="nav-link" data-scroll-target="#tf-idf-for-rating-prediction">TF-IDF for Rating Prediction</a></li>
  </ul></li>
  <li><a href="#tf-idf-with-simple-neural-network-for-rating-prediction" id="toc-tf-idf-with-simple-neural-network-for-rating-prediction" class="nav-link" data-scroll-target="#tf-idf-with-simple-neural-network-for-rating-prediction">TF-IDF with Simple Neural Network for Rating Prediction</a>
  <ul class="collapse">
  <li><a href="#comparison-of-models-for-rating-prediction" id="toc-comparison-of-models-for-rating-prediction" class="nav-link" data-scroll-target="#comparison-of-models-for-rating-prediction">Comparison of Models for Rating Prediction</a></li>
  </ul></li>
  <li><a href="#word-clouds-for-rating-prediction" id="toc-word-clouds-for-rating-prediction" class="nav-link" data-scroll-target="#word-clouds-for-rating-prediction">Word Clouds for Rating Prediction</a></li>
  <li><a href="#bert-for-rating-prediction" id="toc-bert-for-rating-prediction" class="nav-link" data-scroll-target="#bert-for-rating-prediction">BERT for Rating Prediction</a></li>
  <li><a href="#rating-prediction-results" id="toc-rating-prediction-results" class="nav-link" data-scroll-target="#rating-prediction-results">Rating Prediction Results</a></li>
  </ul></li>
  <li><a href="#results-analysis-and-discussion" id="toc-results-analysis-and-discussion" class="nav-link" data-scroll-target="#results-analysis-and-discussion">Results, Analysis, and Discussion</a>
  <ul>
  <li><a href="#summary-of-results" id="toc-summary-of-results" class="nav-link" data-scroll-target="#summary-of-results">Summary of Results</a></li>
  </ul></li>
  <li><a href="#impact" id="toc-impact" class="nav-link" data-scroll-target="#impact">Impact</a>
  <ul>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">CS670 Data Science Project: Sentiment and Rating Analysis of Book Reviews</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Prasaya Acharya and Marvin Chan </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The goal of this project was to analyze the large UCSD BookGraph dataset of Goodreads book reviews and build models for sentiment analysis and rating prediction on those reviews. We were motivated our love of reading and by the rich content on Goodreads where reviews are often long and detailed. The BookGraph dataset allows us to explore how readers express sentiment in text and whether we can predict the star rating of a review from its text alone. We aimed to first conduct exploratory analysis of review characteristics (length, genres, etc.), then classify reviews as positive or negative, and finally predict the exact star rating (1–5) from the review text. By comparing different approaches, we aim to assess what techniques perform best on long-form book reviews and what insights can be gained from this literature-centric data.</p>
</section>
<section id="dataset" class="level2">
<h2 class="anchored" data-anchor-id="dataset">Dataset</h2>
<p>Our data comes from the <strong>Goodreads Book Reviews</strong> corpus released as part of the UCSD BookGraph project by Wan and McAuley. This dataset contains <strong>15 million reviews</strong> for <strong>2 million books</strong> contributed by <strong>465,000 users</strong>​. The data was collected in the context of studying spoiler detection on Goodreads since Goodreads allows users to tag spoilers in their reviews​. For our purposes, we focused on the review texts and ratings. Each review includes: a textual review, a star rating (0–5 stars, where 0 indicates the user marked the book as not finished or not rated), a book ID (with associated metadata like genres in a separate file), and other info like the number of likes/upvotes on the review.</p>
<p>Given the enormous size of the full dataset, we worked with a random sample of 20,000 reviews drawn from the larger set for our analysis and modeling. We first downloaded the relevant data files (book metadata and reviews) and performed parsing. The raw review file contained ~1.38 million reviews, this was a subset of the larger dataset focused on spoiler-tagged reviews). We loaded these into a DataFrame, merged in the book genre information by book ID, and then filtered the data. Notably, we removed entries with a 0-star rating (about 3% of the data) since these reviews are essentially <em>did-not-finish (DNF)</em> markers and not actual rated opinions​ . After dropping these, the ratings range from 1 to 5 stars. We also processed the text minimally – converting to lowercase and letting our modeling tools handle tokenization. For the Bag-of-Words and TF-IDF approaches, we relied on scikit-learn’s vectorizers with an English stopword list and set a minimum document frequency of 5 to ignore extremely rare words. The book genre data was parsed from a nested JSON into a list of genres for each book. We found 16 top-level genres after processing (e.g.&nbsp;Fiction, Fantasy, Romance, Non-fiction, etc.), which we used to examine genre trends.</p>
<section id="setuppreprocessing" class="level3">
<h3 class="anchored" data-anchor-id="setuppreprocessing">Setup/preprocessing:</h3>
<p>Note: some code relevant to BERT has been commented out as we were unable to run Quarto on Talapas and therefore unable to render this report otherwise. We have manually supplemented the report with the relevant images and outputs.</p>
<div id="ecdb389e" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gzip</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> wordcloud <span class="im">import</span> WordCloud</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer, TfidfVectorizer</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, classification_report, confusion_matrix</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Dense, Dropout</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.utils <span class="im">import</span> to_categorical</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># import torch</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># from torch.utils.data import Dataset, DataLoader</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># import torch.nn as nn</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># import torch.optim as optim</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Download necessary NLTK data (if not already downloaded)</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    stopwords.words(<span class="st">'english'</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">LookupError</span>:</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    nltk.download(<span class="st">'stopwords'</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    word_tokenize(<span class="st">"example"</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">LookupError</span>:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    nltk.download(<span class="st">'punkt'</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data and sample - adjust file paths as necessary</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming data is in the same directory or adjust paths accordingly</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>reviews_file <span class="op">=</span> <span class="st">'goodreads_reviews_spoiler_raw.json'</span> <span class="co"># Or 'reviews_sample.json' if you have a smaller sample</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>books_file <span class="op">=</span> <span class="st">'goodreads_book_genres_initial.json'</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Load a small sample for faster prototyping, or load full dataset for comprehensive results</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="co"># reviews = pd.read_json(reviews_file, lines=True, nrows=20000) # Sample load, faster for testing</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>reviews <span class="op">=</span> pd.read_json(reviews_file, lines<span class="op">=</span><span class="va">True</span>) <span class="co"># Full load for comprehensive analysis</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>books <span class="op">=</span> pd.read_json(books_file, lines<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop 0 ratings (DNFs)</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>reviews <span class="op">=</span> reviews[reviews[<span class="st">'rating'</span>] <span class="op">!=</span> <span class="dv">0</span>]</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to parse genres</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parse_genres(genre_dict):</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(genre_dict, <span class="bu">dict</span>):</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> []</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    all_genres <span class="op">=</span> []</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> genre_group <span class="kw">in</span> genre_dict.keys():</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>        individual_genres <span class="op">=</span> [g.strip() <span class="cf">for</span> g <span class="kw">in</span> genre_group.split(<span class="st">','</span>)]</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>        all_genres.extend(individual_genres)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> all_genres</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>books[<span class="st">'parsed_genres'</span>] <span class="op">=</span> books[<span class="st">'genres'</span>].<span class="bu">apply</span>(parse_genres)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>books.drop(columns<span class="op">=</span>[<span class="st">"genres"</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>books.rename(columns<span class="op">=</span>{<span class="st">"parsed_genres"</span>: <span class="st">"genres"</span>}, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample reviews for faster processing during development - consider removing for final run</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>reviews_sample <span class="op">=</span> reviews.sample(n<span class="op">=</span><span class="dv">20000</span>, random_state<span class="op">=</span><span class="dv">42</span>) <span class="co"># Sampling 20000 reviews</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> reviews_sample.copy() <span class="co"># Use the sample DataFrame for analysis</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="co"># Merge reviews and books dataframes</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>df_merged <span class="op">=</span> pd.merge(df, books, on<span class="op">=</span><span class="st">'book_id'</span>, how<span class="op">=</span><span class="st">'inner'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>2025-03-21 18:23:57.749552: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-21 18:23:57.750320: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-03-21 18:23:57.753475: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-03-21 18:23:57.760603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742606637.773096  647939 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742606637.776440  647939 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1742606637.785112  647939 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742606637.785134  647939 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742606637.785135  647939 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742606637.785136  647939 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-03-21 18:23:57.788332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.</code></pre>
</div>
</div>
</section>
</section>
<section id="exploratory-analysis" class="level2">
<h2 class="anchored" data-anchor-id="exploratory-analysis">Exploratory Analysis</h2>
<p>Before building models, we performed an Exploratory Data Analysis (EDA) to understand the characteristics of the reviews. We first explored the distribution of ratings, the relationship between rating and review length, and the average rating across different genres. This initial exploration provides context for our modeling efforts and helps us understand the characteristics of the book review data.</p>
<section id="distribution-of-ratings" class="level3">
<h3 class="anchored" data-anchor-id="distribution-of-ratings">Distribution of Ratings</h3>
<p>We examined the distribution of star ratings to understand the overall sentiment balance in the dataset. Most reviews are positive, with 4 and 5-star ratings being the most frequent. This positive skew suggests that Goodreads users tend to rate books favorably perhaps because they choose books they expect to enjoy or because extremely low ratings are less common to share. We also note that Goodreads allows a 0-star rating to mark unfinished books. About 3.4% of entries were 0, which we dropped as they are not true ratings.</p>
<div id="8d250960" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Count the occurrences of each rating</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>rating_counts <span class="op">=</span> df[<span class="st">'rating'</span>].value_counts().sort_index()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure and bar plot</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.barplot(x<span class="op">=</span>rating_counts.index, y<span class="op">=</span>rating_counts.values, palette<span class="op">=</span><span class="st">"viridis"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add value labels on top of each bar</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, count <span class="kw">in</span> <span class="bu">enumerate</span>(rating_counts.values):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    ax.text(i, count <span class="op">+</span> (count<span class="op">*</span><span class="fl">0.01</span>), <span class="ss">f'</span><span class="sc">{</span>count<span class="sc">:,}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Add titles and labels</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Distribution of Book Ratings'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Rating'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Count'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>plt.xticks(fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>plt.yticks(fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust layout and display</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Print rating distribution statistics</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total reviews: </span><span class="sc">{</span><span class="bu">len</span>(df)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Rating distribution percentage:"</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> rating, count <span class="kw">in</span> rating_counts.items():</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Rating </span><span class="sc">{</span>rating<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>count<span class="op">/</span><span class="bu">len</span>(df) <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_647939/3783421683.py:6: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  ax = sns.barplot(x=rating_counts.index, y=rating_counts.values, palette="viridis")</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-3-output-2.png" width="951" height="567" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Total reviews: 20000
Rating distribution percentage:
Rating 1: 3.37%
Rating 2: 8.66%
Rating 3: 21.98%
Rating 4: 35.83%
Rating 5: 30.17%</code></pre>
</div>
</div>
</section>
<section id="rating-vs.-review-text-word-count" class="level3">
<h3 class="anchored" data-anchor-id="rating-vs.-review-text-word-count">Rating vs.&nbsp;Review Text Word Count</h3>
<p>We next examined the length of reviews and whether longer reviews correlate with higher or lower ratings. To see if people write more when they love or hate a book, we created a scatter plot of ratings against the word count of reviews. The plot shows us that there is no statistically significant relation between these two variables, but there is a visual trend in the plot where it appears that the longest reviews tend to have higher ratings of the book. This suggests that review length is more of a personal style or how much a person has to say about the book, rather than directly to sentiment.</p>
<div id="67d00071" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate word count for each review</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'word_count'</span>] <span class="op">=</span> df[<span class="st">'review_text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(<span class="bu">str</span>(x).split()))</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure with appropriate size</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data for scatter plot to improve performance and readability</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>sample_size <span class="op">=</span> <span class="bu">min</span>(<span class="dv">10000</span>, <span class="bu">len</span>(df))</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>reviews_sample <span class="op">=</span> df.sample(sample_size, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create scatter plot with transparency</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(x<span class="op">=</span><span class="st">'rating'</span>, y<span class="op">=</span><span class="st">'word_count'</span>, data<span class="op">=</span>reviews_sample, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Add trend line</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">'rating'</span>, y<span class="op">=</span><span class="st">'word_count'</span>, data<span class="op">=</span>df, scatter<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">'red'</span>, line_kws<span class="op">=</span>{<span class="st">'linewidth'</span>: <span class="dv">2</span>})</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Add labels and title</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Rating vs Review Word Count'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Rating'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Word Count'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Set x-axis to show ratings as discrete values</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate and display correlation coefficient</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>correlation <span class="op">=</span> df[[<span class="st">'rating'</span>, <span class="st">'word_count'</span>]].corr().iloc[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="ss">f'Correlation: </span><span class="sc">{</span>correlation<span class="sc">:.3f}</span><span class="ss">'</span>,</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>             xy<span class="op">=</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>), xycoords<span class="op">=</span><span class="st">'axes fraction'</span>,</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>             fontsize<span class="op">=</span><span class="dv">12</span>, bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">"round,pad=0.3"</span>, fc<span class="op">=</span><span class="st">"white"</span>, ec<span class="op">=</span><span class="st">"gray"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>))</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mean word count by rating and add to plot</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>mean_counts <span class="op">=</span> df.groupby(<span class="st">'rating'</span>)[<span class="st">'word_count'</span>].mean()</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> rating, mean_count <span class="kw">in</span> mean_counts.items():</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    plt.annotate(<span class="ss">f'Mean: </span><span class="sc">{</span>mean_count<span class="sc">:.0f}</span><span class="ss">'</span>,</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>                 xy<span class="op">=</span>(rating, mean_count),</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>                 xytext<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">10</span>), textcoords<span class="op">=</span><span class="st">'offset points'</span>,</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>                 ha<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>                 bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">"round,pad=0.2"</span>, fc<span class="op">=</span><span class="st">"white"</span>, ec<span class="op">=</span><span class="st">"gray"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>))</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Add grid for better readability</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Tight layout and show plot</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Print word count statistics by rating</span></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Word count statistics by rating:"</span>)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> rating, group <span class="kw">in</span> df.groupby(<span class="st">'rating'</span>):</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Rating </span><span class="sc">{</span>rating<span class="sc">}</span><span class="ss">: Mean=</span><span class="sc">{</span>group[<span class="st">'word_count'</span>]<span class="sc">.</span>mean()<span class="sc">:.1f}</span><span class="ss">, Median=</span><span class="sc">{</span>group[<span class="st">'word_count'</span>]<span class="sc">.</span>median()<span class="sc">}</span><span class="ss">, Min=</span><span class="sc">{</span>group[<span class="st">'word_count'</span>]<span class="sc">.</span><span class="bu">min</span>()<span class="sc">}</span><span class="ss">, Max=</span><span class="sc">{</span>group[<span class="st">'word_count'</span>]<span class="sc">.</span><span class="bu">max</span>()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-4-output-1.png" width="1143" height="759" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Word count statistics by rating:
Rating 1: Mean=198.8, Median=112.5, Min=1, Max=1563
Rating 2: Mean=211.1, Median=128.0, Min=1, Max=1677
Rating 3: Mean=186.4, Median=112.0, Min=1, Max=2500
Rating 4: Mean=200.3, Median=119.0, Min=1, Max=2064
Rating 5: Mean=207.5, Median=113.0, Min=1, Max=2005</code></pre>
</div>
</div>
</section>
<section id="average-rating-per-genre" class="level3">
<h3 class="anchored" data-anchor-id="average-rating-per-genre">Average Rating per Genre</h3>
<p>We then examined average book ratings across different genres.Each book in the dataset comes with genre tags, so we identified the set of distinct genres and counted reviews per genre. We looked at whether certain genres tend to get higher or lower ratings on average. We calculated the average rating for each genre and visualized these averages using a horizontal bar plot, sorted by average rating in descending order. We can see that genres like nonfiction and historical fiction are lower rated, while genres like comics and childrens’ books are rated highest, but the genre difference in ratings is minimal.</p>
<div id="e9201844" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Explode genres list to have one genre per row</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>exploded_data <span class="op">=</span> df_merged.explode(<span class="st">'genres'</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Group by genre and calculate average rating and count</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>genre_ratings <span class="op">=</span> exploded_data.groupby(<span class="st">'genres'</span>)[<span class="st">'rating'</span>].agg([<span class="st">'mean'</span>, <span class="st">'count'</span>]).sort_values(by<span class="op">=</span><span class="st">'mean'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>genre_ratings <span class="op">=</span> genre_ratings.reset_index() <span class="co"># Reset index to make 'genres' a column</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure with appropriate size</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Use a Seaborn color palette</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>palette <span class="op">=</span> sns.color_palette(<span class="st">"viridis"</span>, n_colors<span class="op">=</span><span class="bu">len</span>(genre_ratings))</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create horizontal bar plot</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>bars <span class="op">=</span> plt.barh(genre_ratings[<span class="st">'genres'</span>], genre_ratings[<span class="st">'mean'</span>], color<span class="op">=</span>palette)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>plt.gca().invert_yaxis() <span class="co"># Invert y-axis to have highest rated genre at top</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Add values on the bars</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, bar <span class="kw">in</span> <span class="bu">enumerate</span>(bars):</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    plt.text(bar.get_width() <span class="op">+</span> <span class="fl">0.05</span>,</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>             bar.get_y() <span class="op">+</span> bar.get_height()<span class="op">/</span><span class="dv">2</span>,</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>             <span class="ss">f'</span><span class="sc">{</span>genre_ratings[<span class="st">"mean"</span>]<span class="sc">.</span>iloc[i]<span class="sc">:.2f}</span><span class="ss"> (</span><span class="sc">{</span>genre_ratings[<span class="st">"count"</span>]<span class="sc">.</span>iloc[i]<span class="sc">:,}</span><span class="ss">)'</span>,</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>             va<span class="op">=</span><span class="st">'center'</span>,</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>             fontweight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>             color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Set labels and title</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Average Rating'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Genre'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Average Book Ratings by Genre'</span>, fontsize<span class="op">=</span><span class="dv">18</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="fl">5.5</span>) <span class="co"># Set x-axis limit for better visualization</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Style grid</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>plt.grid(axis<span class="op">=</span><span class="st">'x'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Style tick labels</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>plt.xticks(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>plt.yticks(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust layout and display</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Print genre rating statistics</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of genres: </span><span class="sc">{</span><span class="bu">len</span>(genre_ratings)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Genre with highest average rating: </span><span class="sc">{</span>genre_ratings<span class="sc">.</span>iloc[<span class="dv">0</span>][<span class="st">'genres'</span>]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>genre_ratings<span class="sc">.</span>iloc[<span class="dv">0</span>][<span class="st">'mean'</span>]<span class="sc">:.2f}</span><span class="ss">)"</span>)</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Genre with lowest average rating: </span><span class="sc">{</span>genre_ratings<span class="sc">.</span>iloc[<span class="op">-</span><span class="dv">1</span>][<span class="st">'genres'</span>]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>genre_ratings<span class="sc">.</span>iloc[<span class="op">-</span><span class="dv">1</span>][<span class="st">'mean'</span>]<span class="sc">:.2f}</span><span class="ss">)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-5-output-1.png" width="1334" height="948" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of genres: 16
Genre with highest average rating: children (3.88)
Genre with lowest average rating: non-fiction (3.74)</code></pre>
</div>
</div>
</section>
</section>
<section id="sentiment-analysis" class="level2">
<h2 class="anchored" data-anchor-id="sentiment-analysis">Sentiment Analysis</h2>
<p>Our first modeling task was sentiment analysis, where we aimed to classify book reviews as either positive or negative based on their text. For sentiment analysis, we labeled reviews with rating 4 or 5 as positive and rating 1, 2, or 3 as negative. We experimented with Bag-of-Words and TF-IDF vectorization combined with Logistic Regression and Simple Neural Networks, and then fine tuned a DistilBERT-based model for comparison.</p>
<section id="bag-of-words-and-tf-idf-with-logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="bag-of-words-and-tf-idf-with-logistic-regression">Bag of Words and TF-IDF with Logistic Regression</h3>
<p>We used Bag-of-Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) to convert the review texts into numerical vectors.</p>
<p>In a BoW model, each review is represented by features indicating the presence or frequency of words, disregarding order. We limited the vocabulary to the 5,000 most frequent words in the corpus to reduce dimensionality and ignored words that appeared in fewer than 5 reviews to drop rare outliers. Next, we used TF‑IDF features instead of raw word counts. Term Frequency–Inverse Document Frequency weighs words by how unique they are to a document i.e.&nbsp;words that appear frequently in a review but not in many other reviews get a higher weight. This can help down-weight very common words.</p>
<p>These vectors were then used to train Logistic Regression classifiers. We evaluated performance using accuracy, classification reports, and confusion matrices. We also analyzed feature importance to identify the words most indicative of positive and negative sentiment.</p>
<section id="bag-of-words-model" class="level4">
<h4 class="anchored" data-anchor-id="bag-of-words-model">Bag of Words Model</h4>
<div id="8c64153b" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define sentiment based on rating (positive if rating &gt; 3, negative otherwise)</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'sentiment'</span>] <span class="op">=</span> df[<span class="st">'rating'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">'positive'</span> <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">3</span> <span class="cf">else</span> <span class="st">'negative'</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data into training and testing sets</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'review_text'</span>],</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'sentiment'</span>].<span class="bu">map</span>({<span class="st">'positive'</span>: <span class="dv">1</span>, <span class="st">'negative'</span>: <span class="dv">0</span>}), <span class="co"># Map sentiment to numerical labels</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Pipeline with CountVectorizer and Logistic Regression</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>pipeline_bow <span class="op">=</span> Pipeline([</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'vectorizer'</span>, CountVectorizer(max_features<span class="op">=</span><span class="dv">5000</span>, min_df<span class="op">=</span><span class="dv">5</span>, stop_words<span class="op">=</span><span class="st">'english'</span>)), <span class="co"># Bag of Words vectorization</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'classifier'</span>, LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>, C<span class="op">=</span><span class="fl">1.0</span>)) <span class="co"># Logistic Regression classifier</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>pipeline_bow.fit(X_train, y_train)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>y_pred_bow <span class="op">=</span> pipeline_bow.predict(X_test)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>accuracy_bow_logistic <span class="op">=</span> accuracy_score(y_test, y_pred_bow)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy (Bag of Words + Logistic Regression): </span><span class="sc">{</span>accuracy_bow_logistic<span class="sc">:.5f}</span><span class="ss">"</span>)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification Report:"</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred_bow))</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Get confusion matrix</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>cm_bow <span class="op">=</span> confusion_matrix(y_test, y_pred_bow)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>sns.heatmap(cm_bow, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'d'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>,</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>            xticklabels<span class="op">=</span>[<span class="st">'Negative'</span>, <span class="st">'Positive'</span>],</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>            yticklabels<span class="op">=</span>[<span class="st">'Negative'</span>, <span class="st">'Positive'</span>])</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted'</span>)</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True'</span>)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix: Bag of Words + Logistic Regression'</span>)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature Importance Analysis for Bag of Words Model</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>vectorizer_bow <span class="op">=</span> pipeline_bow.named_steps[<span class="st">'vectorizer'</span>]</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>classifier_bow <span class="op">=</span> pipeline_bow.named_steps[<span class="st">'classifier'</span>]</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>feature_names_bow <span class="op">=</span> vectorizer_bow.get_feature_names_out()</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>coefs_bow <span class="op">=</span> classifier_bow.coef_[<span class="dv">0</span>]</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Top negative and positive words</span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>top_negative_words_bow <span class="op">=</span> pd.DataFrame({<span class="st">'word'</span>: [feature_names_bow[idx] <span class="cf">for</span> idx <span class="kw">in</span> coefs_bow.argsort()[:<span class="dv">25</span>]], <span class="st">'Coefficient'</span>: <span class="bu">sorted</span>(coefs_bow)[:<span class="dv">25</span>]})</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>top_positive_words_bow <span class="op">=</span> pd.DataFrame({<span class="st">'word'</span>: [feature_names_bow[idx] <span class="cf">for</span> idx <span class="kw">in</span> coefs_bow.argsort()[<span class="op">-</span><span class="dv">25</span>:]][::<span class="op">-</span><span class="dv">1</span>], <span class="st">'Coefficient'</span>: <span class="bu">sorted</span>(coefs_bow)[<span class="op">-</span><span class="dv">25</span>:][::<span class="op">-</span><span class="dv">1</span>]})</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top negative words (Bag of Words):"</span>)</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>display(top_negative_words_bow)</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top positive words (Bag of Words):"</span>)</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>display(top_positive_words_bow)</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Word Cloud for Negative Sentiment (Bag of Words)</span></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>negative_word_importance_bow <span class="op">=</span> {top_negative_words_bow[<span class="st">'word'</span>][i]: <span class="bu">abs</span>(top_negative_words_bow[<span class="st">'Coefficient'</span>][i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(top_negative_words_bow))}</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>wordcloud_neg_bow <span class="op">=</span> WordCloud(width<span class="op">=</span><span class="dv">1000</span>, height<span class="op">=</span><span class="dv">1000</span>, background_color<span class="op">=</span><span class="st">'white'</span>, max_words<span class="op">=</span><span class="dv">150</span>, colormap<span class="op">=</span><span class="st">'Reds'</span>, prefer_horizontal<span class="op">=</span><span class="fl">1.0</span>).generate_from_frequencies(negative_word_importance_bow)</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>plt.imshow(wordcloud_neg_bow, interpolation<span class="op">=</span><span class="st">'bilinear'</span>)</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Negative Sentiment Word Cloud (Bag of Words)'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Word Cloud for Positive Sentiment (Bag of Words)</span></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>positive_word_importance_bow <span class="op">=</span> {top_positive_words_bow[<span class="st">'word'</span>][i]: top_positive_words_bow[<span class="st">'Coefficient'</span>][i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(top_positive_words_bow))}</span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>wordcloud_pos_bow <span class="op">=</span> WordCloud(width<span class="op">=</span><span class="dv">1000</span>, height<span class="op">=</span><span class="dv">1000</span>, background_color<span class="op">=</span><span class="st">'white'</span>, max_words<span class="op">=</span><span class="dv">150</span>, colormap<span class="op">=</span><span class="st">'Greens'</span>, prefer_horizontal<span class="op">=</span><span class="fl">1.0</span>).generate_from_frequencies(positive_word_importance_bow)</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>plt.imshow(wordcloud_pos_bow, interpolation<span class="op">=</span><span class="st">'bilinear'</span>)</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Positive Sentiment Word Cloud (Bag of Words)'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy (Bag of Words + Logistic Regression): 0.74425

Classification Report:
              precision    recall  f1-score   support

           0       0.64      0.56      0.60      1355
           1       0.79      0.84      0.81      2645

    accuracy                           0.74      4000
   macro avg       0.71      0.70      0.71      4000
weighted avg       0.74      0.74      0.74      4000
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-6-output-2.png" width="721" height="566" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top negative words (Bag of Words):</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>meh</td>
<td>-2.402060</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>dnf</td>
<td>-2.382211</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>cup</td>
<td>-2.354525</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>attempts</td>
<td>-1.830230</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>switched</td>
<td>-1.806441</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>pointless</td>
<td>-1.765946</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>lawyer</td>
<td>-1.711502</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>oldest</td>
<td>-1.709428</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>waste</td>
<td>-1.664098</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>disappointing</td>
<td>-1.628762</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>immature</td>
<td>-1.545206</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>sequels</td>
<td>-1.541481</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>kira</td>
<td>-1.514262</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>unfortunately</td>
<td>-1.493600</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>potential</td>
<td>-1.468826</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15</td>
<td>tedious</td>
<td>-1.465438</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">16</td>
<td>obnoxious</td>
<td>-1.459581</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">17</td>
<td>skimming</td>
<td>-1.459031</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">18</td>
<td>bullshit</td>
<td>-1.443726</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">19</td>
<td>unrealistic</td>
<td>-1.430920</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">20</td>
<td>uninteresting</td>
<td>-1.427234</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">21</td>
<td>bland</td>
<td>-1.424752</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">22</td>
<td>desires</td>
<td>-1.423058</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">23</td>
<td>flat</td>
<td>-1.378483</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">24</td>
<td>alright</td>
<td>-1.361102</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top positive words (Bag of Words):</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>cried</td>
<td>2.119104</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>gritty</td>
<td>1.824090</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>trade</td>
<td>1.684277</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>rude</td>
<td>1.663065</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>tall</td>
<td>1.617877</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>stress</td>
<td>1.611858</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>perkins</td>
<td>1.602474</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>speechless</td>
<td>1.583741</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>immensely</td>
<td>1.558319</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>exposed</td>
<td>1.549780</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>spanish</td>
<td>1.528739</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>hug</td>
<td>1.518261</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>constructed</td>
<td>1.509862</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>shift</td>
<td>1.483670</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>legends</td>
<td>1.457647</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15</td>
<td>screaming</td>
<td>1.447419</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">16</td>
<td>delicious</td>
<td>1.427741</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">17</td>
<td>predict</td>
<td>1.426215</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">18</td>
<td>spoil</td>
<td>1.417543</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">19</td>
<td>ordered</td>
<td>1.382787</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">20</td>
<td>nails</td>
<td>1.382539</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">21</td>
<td>favorites</td>
<td>1.368814</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">22</td>
<td>tears</td>
<td>1.360918</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">23</td>
<td>breathtaking</td>
<td>1.351326</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">24</td>
<td>spunky</td>
<td>1.346960</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-6-output-7.png" width="654" height="639" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-6-output-8.png" width="637" height="639" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="tf-idf-model" class="level4">
<h4 class="anchored" data-anchor-id="tf-idf-model">TF-IDF Model</h4>
<div id="a75ecb31" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pipeline with TfidfVectorizer and Logistic Regression</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>pipeline_tfidf <span class="op">=</span> Pipeline([</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'vectorizer'</span>, TfidfVectorizer(max_features<span class="op">=</span><span class="dv">5000</span>, min_df<span class="op">=</span><span class="dv">5</span>, stop_words<span class="op">=</span><span class="st">'english'</span>)), <span class="co"># TF-IDF vectorization</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'classifier'</span>, LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>, C<span class="op">=</span><span class="fl">1.0</span>)) <span class="co"># Logistic Regression classifier</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the TF-IDF model</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>pipeline_tfidf.fit(X_train, y_train)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>y_pred_tfidf <span class="op">=</span> pipeline_tfidf.predict(X_test)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the TF-IDF model</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>accuracy_tfidf_logistic <span class="op">=</span> accuracy_score(y_test, y_pred_tfidf)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy (TF-IDF + Logistic Regression): </span><span class="sc">{</span>accuracy_tfidf_logistic<span class="sc">:.5f}</span><span class="ss">"</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification Report:"</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred_tfidf))</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion Matrix for TF-IDF Model</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>cm_tfidf <span class="op">=</span> confusion_matrix(y_test, y_pred_tfidf)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>sns.heatmap(cm_tfidf, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'d'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>,</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>            xticklabels<span class="op">=</span>[<span class="st">'Negative'</span>, <span class="st">'Positive'</span>],</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>            yticklabels<span class="op">=</span>[<span class="st">'Negative'</span>, <span class="st">'Positive'</span>])</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted'</span>)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True'</span>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix: TF-IDF + Logistic Regression'</span>)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature Importance Analysis for TF-IDF Model</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>vectorizer_tfidf <span class="op">=</span> pipeline_tfidf.named_steps[<span class="st">'vectorizer'</span>]</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>classifier_tfidf <span class="op">=</span> pipeline_tfidf.named_steps[<span class="st">'classifier'</span>]</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>feature_names_tfidf <span class="op">=</span> vectorizer_tfidf.get_feature_names_out()</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>coefs_tfidf <span class="op">=</span> classifier_tfidf.coef_[<span class="dv">0</span>]</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Top negative and positive words for TF-IDF</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>top_negative_words_tfidf <span class="op">=</span> pd.DataFrame({<span class="st">'word'</span>: [feature_names_tfidf[idx] <span class="cf">for</span> idx <span class="kw">in</span> coefs_tfidf.argsort()[:<span class="dv">25</span>]], <span class="st">'Coefficient'</span>: <span class="bu">sorted</span>(coefs_tfidf)[:<span class="dv">25</span>]})</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>top_positive_words_tfidf <span class="op">=</span> pd.DataFrame({<span class="st">'word'</span>: [feature_names_tfidf[idx] <span class="cf">for</span> idx <span class="kw">in</span> coefs_tfidf.argsort()[<span class="op">-</span><span class="dv">25</span>:]][::<span class="op">-</span><span class="dv">1</span>], <span class="st">'Coefficient'</span>: <span class="bu">sorted</span>(coefs_tfidf)[<span class="op">-</span><span class="dv">25</span>:][::<span class="op">-</span><span class="dv">1</span>]})</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top negative words (TF-IDF):"</span>)</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>display(top_negative_words_tfidf)</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top positive words (TF-IDF):"</span>)</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>display(top_positive_words_tfidf)</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Word Cloud for Negative Sentiment (TF-IDF)</span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>negative_word_importance_tfidf <span class="op">=</span> {top_negative_words_tfidf[<span class="st">'word'</span>][i]: <span class="bu">abs</span>(top_negative_words_tfidf[<span class="st">'Coefficient'</span>][i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(top_negative_words_tfidf))}</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>wordcloud_neg_tfidf <span class="op">=</span> WordCloud(width<span class="op">=</span><span class="dv">1000</span>, height<span class="op">=</span><span class="dv">1000</span>, background_color<span class="op">=</span><span class="st">'white'</span>, max_words<span class="op">=</span><span class="dv">150</span>, colormap<span class="op">=</span><span class="st">'Reds'</span>, prefer_horizontal<span class="op">=</span><span class="fl">1.0</span>).generate_from_frequencies(negative_word_importance_tfidf)</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>plt.imshow(wordcloud_neg_tfidf, interpolation<span class="op">=</span><span class="st">'bilinear'</span>)</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Negative Sentiment Word Cloud (TF-IDF)'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Word Cloud for Positive Sentiment (TF-IDF)</span></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>positive_word_importance_tfidf <span class="op">=</span> {top_positive_words_tfidf[<span class="st">'word'</span>][i]: top_positive_words_tfidf[<span class="st">'Coefficient'</span>][i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(top_positive_words_tfidf))}</span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>wordcloud_pos_tfidf <span class="op">=</span> WordCloud(width<span class="op">=</span><span class="dv">1000</span>, height<span class="op">=</span><span class="dv">1000</span>, background_color<span class="op">=</span><span class="st">'white'</span>, max_words<span class="op">=</span><span class="dv">150</span>, colormap<span class="op">=</span><span class="st">'Greens'</span>, prefer_horizontal<span class="op">=</span><span class="fl">1.0</span>).generate_from_frequencies(positive_word_importance_tfidf)</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>plt.imshow(wordcloud_pos_tfidf, interpolation<span class="op">=</span><span class="st">'bilinear'</span>)</span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Positive Sentiment Word Cloud (TF-IDF)'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy (TF-IDF + Logistic Regression): 0.77675

Classification Report:
              precision    recall  f1-score   support

           0       0.74      0.53      0.62      1355
           1       0.79      0.90      0.84      2645

    accuracy                           0.78      4000
   macro avg       0.76      0.72      0.73      4000
weighted avg       0.77      0.78      0.77      4000
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-7-output-2.png" width="721" height="566" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top negative words (TF-IDF):</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>didn</td>
<td>-3.913666</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>boring</td>
<td>-3.822779</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>unfortunately</td>
<td>-3.460737</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>flat</td>
<td>-3.355971</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>dnf</td>
<td>-3.320909</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>meh</td>
<td>-3.308076</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>wasn</td>
<td>-3.282326</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>disappointing</td>
<td>-3.185986</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>disappointed</td>
<td>-3.168249</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>ok</td>
<td>-2.979167</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>potential</td>
<td>-2.740243</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>bad</td>
<td>-2.715018</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>connect</td>
<td>-2.635009</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>bored</td>
<td>-2.574130</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>supposed</td>
<td>-2.548874</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15</td>
<td>waste</td>
<td>-2.514175</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">16</td>
<td>decent</td>
<td>-2.514048</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">17</td>
<td>awful</td>
<td>-2.382318</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">18</td>
<td>okay</td>
<td>-2.359856</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">19</td>
<td>blah</td>
<td>-2.333572</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">20</td>
<td>annoying</td>
<td>-2.293741</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">21</td>
<td>don</td>
<td>-2.293680</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">22</td>
<td>hated</td>
<td>-2.266803</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">23</td>
<td>bland</td>
<td>-2.222294</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">24</td>
<td>felt</td>
<td>-2.194860</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top positive words (TF-IDF):</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>loved</td>
<td>5.070427</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>wait</td>
<td>3.556711</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>love</td>
<td>3.293545</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>beautiful</td>
<td>3.114244</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>great</td>
<td>3.085211</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>heart</td>
<td>3.042890</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>amazing</td>
<td>3.039279</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>highly</td>
<td>2.972992</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>perfect</td>
<td>2.968983</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>excellent</td>
<td>2.578456</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>cried</td>
<td>2.448080</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>fantastic</td>
<td>2.446967</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>wow</td>
<td>2.398308</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>brilliant</td>
<td>2.360806</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>awesome</td>
<td>2.302345</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15</td>
<td>wonderful</td>
<td>2.264723</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">16</td>
<td>absolutely</td>
<td>2.238135</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">17</td>
<td>tears</td>
<td>2.084851</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">18</td>
<td>hilarious</td>
<td>2.054463</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">19</td>
<td>reread</td>
<td>2.029510</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">20</td>
<td>best</td>
<td>2.014448</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">21</td>
<td>beautifully</td>
<td>2.008029</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">22</td>
<td>favorite</td>
<td>1.995542</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">23</td>
<td>happy</td>
<td>1.984921</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">24</td>
<td>holy</td>
<td>1.921845</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-7-output-7.png" width="611" height="639" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-7-output-8.png" width="611" height="639" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="tf-idf-with-simple-neural-network" class="level3">
<h3 class="anchored" data-anchor-id="tf-idf-with-simple-neural-network">TF-IDF with Simple Neural Network</h3>
<p>To explore a more complex model for sentiment classification, we implemented a simple feed-forward Neural Network (NN) using TensorFlow/Keras. The idea was to see if a non-linear model could capture signals that logistic regression (which is linear) might miss. We used TF-IDF features as input to this network and evaluated its performance against Logistic Regression and Bag-of-Words models. In practice, we found the neural network was prone to overfitting the training data quickly, given we had only 16k training examples and 5000 features. We had difficulty running the model on the full dataset due to our hardware memory constraints.</p>
<div id="a1f9da98" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TF-IDF vectorization for Neural Network input</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>vectorizer_nn_sentiment <span class="op">=</span> TfidfVectorizer(max_features<span class="op">=</span><span class="dv">5000</span>, min_df<span class="op">=</span><span class="dv">5</span>, stop_words<span class="op">=</span><span class="st">'english'</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>X_train_tfidf_nn_sentiment <span class="op">=</span> vectorizer_nn_sentiment.fit_transform(X_train).toarray()</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>X_test_tfidf_nn_sentiment <span class="op">=</span> vectorizer_nn_sentiment.transform(X_test).toarray()</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>input_dim_nn_sentiment <span class="op">=</span> X_train_tfidf_nn_sentiment.shape[<span class="dv">1</span>]</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Neural Network model</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>model_nn_sentiment <span class="op">=</span> Sequential([</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>, input_shape<span class="op">=</span>(input_dim_nn_sentiment,)),</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    Dropout(<span class="fl">0.3</span>),</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    Dropout(<span class="fl">0.2</span>),</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>) <span class="co"># Sigmoid for binary classification</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>model_nn_sentiment.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>history_nn_sentiment <span class="op">=</span> model_nn_sentiment.fit(X_train_tfidf_nn_sentiment, y_train,</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>                    epochs<span class="op">=</span><span class="dv">10</span>, batch_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>                    validation_split<span class="op">=</span><span class="fl">0.1</span>, verbose<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>_, accuracy_nn_sentiment <span class="op">=</span> model_nn_sentiment.evaluate(X_test_tfidf_nn_sentiment, y_test, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy (TF-IDF + Neural Network for sentiment analysis): </span><span class="sc">{</span>accuracy_nn_sentiment<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions and Classification Report</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>y_pred_prob_nn_sentiment <span class="op">=</span> model_nn_sentiment.predict(X_test_tfidf_nn_sentiment)</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>y_pred_nn_sentiment <span class="op">=</span> (y_pred_prob_nn_sentiment <span class="op">&gt;</span> <span class="fl">0.5</span>).astype(<span class="bu">int</span>).flatten()</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification Report:"</span>)</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred_nn_sentiment))</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion Matrix for Neural Network</span></span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>cm_nn_sentiment <span class="op">=</span> confusion_matrix(y_test, y_pred_nn_sentiment)</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>sns.heatmap(cm_nn_sentiment, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'d'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>,</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>            xticklabels<span class="op">=</span>[<span class="st">'Negative'</span>, <span class="st">'Positive'</span>],</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>            yticklabels<span class="op">=</span>[<span class="st">'Negative'</span>, <span class="st">'Positive'</span>])</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted'</span>)</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True'</span>)</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix: TF-IDF + Neural Network for Sentiment Analysis'</span>)</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training history</span></span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot accuracy</span></span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>plt.plot(history_nn_sentiment.history[<span class="st">'accuracy'</span>], label<span class="op">=</span><span class="st">'Train Accuracy'</span>)</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>plt.plot(history_nn_sentiment.history[<span class="st">'val_accuracy'</span>], label<span class="op">=</span><span class="st">'Validation Accuracy'</span>)</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Accuracy'</span>)</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot loss</span></span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>plt.plot(history_nn_sentiment.history[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'Train Loss'</span>)</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>plt.plot(history_nn_sentiment.history[<span class="st">'val_loss'</span>], label<span class="op">=</span><span class="st">'Validation Loss'</span>)</span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Loss'</span>)</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature Importance - using weights from the first layer of the NN</span></span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>weights_nn_sentiment <span class="op">=</span> model_nn_sentiment.layers[<span class="dv">0</span>].get_weights()[<span class="dv">0</span>]</span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a>feature_importance_nn_sentiment <span class="op">=</span> np.mean(np.<span class="bu">abs</span>(weights_nn_sentiment), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a>top_indices_nn_sentiment <span class="op">=</span> feature_importance_nn_sentiment.argsort()[<span class="op">-</span><span class="dv">30</span>:][::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a>df_importance_nn_sentiment <span class="op">=</span> pd.DataFrame({</span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Word'</span>: [vectorizer_nn_sentiment.get_feature_names_out()[idx] <span class="cf">for</span> idx <span class="kw">in</span> top_indices_nn_sentiment],</span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Importance'</span>: feature_importance_nn_sentiment[top_indices_nn_sentiment]</span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">12</span>))</span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a>sns.barplot(x<span class="op">=</span><span class="st">'Importance'</span>, y<span class="op">=</span><span class="st">'Word'</span>, data<span class="op">=</span>df_importance_nn_sentiment, palette<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Neural Network Feature Importance'</span>, fontsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Mean Absolute Weight'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/home/default/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
E0000 00:00:1742606770.198064  647939 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.
W0000 00:00:1742606770.198442  647939 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
  1/225 ━━━━━━━━━━━━━━━━━━━━ 2:10 582ms/step - accuracy: 0.4531 - loss: 0.6953 13/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5675 - loss: 0.6886     25/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6023 - loss: 0.6800 37/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6162 - loss: 0.6737 49/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6229 - loss: 0.6690 61/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6276 - loss: 0.6643 74/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6318 - loss: 0.6590 86/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6349 - loss: 0.6543 98/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6379 - loss: 0.6493112/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6418 - loss: 0.6434126/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6458 - loss: 0.6378141/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6503 - loss: 0.6319155/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6542 - loss: 0.6268168/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6577 - loss: 0.6222181/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6610 - loss: 0.6179195/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6644 - loss: 0.6135209/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6677 - loss: 0.6093222/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6706 - loss: 0.6056225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.6715 - loss: 0.6045 - val_accuracy: 0.7831 - val_loss: 0.4703
Epoch 2/10
  1/225 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.8281 - loss: 0.3840 16/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8377 - loss: 0.3865  30/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8396 - loss: 0.3773 44/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8400 - loss: 0.3732 56/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8388 - loss: 0.3724 69/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8376 - loss: 0.3730 81/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8373 - loss: 0.3731 93/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8370 - loss: 0.3734105/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8365 - loss: 0.3743117/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8358 - loss: 0.3754128/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8352 - loss: 0.3765139/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8347 - loss: 0.3774151/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8342 - loss: 0.3783163/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8336 - loss: 0.3792175/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8330 - loss: 0.3800187/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8326 - loss: 0.3807199/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8322 - loss: 0.3814212/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8317 - loss: 0.3820224/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8313 - loss: 0.3826225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8312 - loss: 0.3827 - val_accuracy: 0.7719 - val_loss: 0.4746
Epoch 3/10
  1/225 ━━━━━━━━━━━━━━━━━━━━ 5s 23ms/step - accuracy: 0.8594 - loss: 0.3702 14/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8748 - loss: 0.3288  27/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8737 - loss: 0.3243 42/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8738 - loss: 0.3189 56/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8749 - loss: 0.3147 70/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8768 - loss: 0.3101 83/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8777 - loss: 0.3076 96/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8783 - loss: 0.3059110/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8786 - loss: 0.3048124/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8786 - loss: 0.3041137/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8785 - loss: 0.3039150/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8783 - loss: 0.3039163/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8781 - loss: 0.3040177/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8777 - loss: 0.3043191/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8773 - loss: 0.3047205/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8768 - loss: 0.3051220/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8763 - loss: 0.3056225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.8761 - loss: 0.3058 - val_accuracy: 0.7638 - val_loss: 0.5279
Epoch 4/10
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9688 - loss: 0.1785 14/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9386 - loss: 0.2128  26/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9342 - loss: 0.2137 38/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9319 - loss: 0.2128 51/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9309 - loss: 0.2119 64/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9304 - loss: 0.2113 77/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9300 - loss: 0.2107 89/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9295 - loss: 0.2106102/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9289 - loss: 0.2109116/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9282 - loss: 0.2112130/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9277 - loss: 0.2114145/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9272 - loss: 0.2114159/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9268 - loss: 0.2113173/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9265 - loss: 0.2112187/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9262 - loss: 0.2113201/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9258 - loss: 0.2114216/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9253 - loss: 0.2118225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9250 - loss: 0.2121 - val_accuracy: 0.7550 - val_loss: 0.6131
Epoch 5/10
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9844 - loss: 0.1422 17/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9690 - loss: 0.1320  33/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9659 - loss: 0.1303 48/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9635 - loss: 0.1311 64/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9627 - loss: 0.1301 79/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9622 - loss: 0.1296 92/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9619 - loss: 0.1293105/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9618 - loss: 0.1290118/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9616 - loss: 0.1287131/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9615 - loss: 0.1283145/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9613 - loss: 0.1282159/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9612 - loss: 0.1280172/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9610 - loss: 0.1278185/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9609 - loss: 0.1278197/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9607 - loss: 0.1278209/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9605 - loss: 0.1278221/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9603 - loss: 0.1279225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9602 - loss: 0.1279 - val_accuracy: 0.7563 - val_loss: 0.7703
Epoch 6/10
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9844 - loss: 0.0689 14/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9885 - loss: 0.0553  27/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9855 - loss: 0.0595 41/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9846 - loss: 0.0611 55/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9840 - loss: 0.0615 69/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9835 - loss: 0.0621 83/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9832 - loss: 0.0622 97/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9829 - loss: 0.0623111/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9827 - loss: 0.0624125/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9824 - loss: 0.0627139/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9822 - loss: 0.0630152/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9820 - loss: 0.0632166/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9819 - loss: 0.0633180/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9817 - loss: 0.0635194/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9815 - loss: 0.0637207/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9813 - loss: 0.0638221/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9812 - loss: 0.0640225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9811 - loss: 0.0640 - val_accuracy: 0.7494 - val_loss: 0.9483
Epoch 7/10
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 19ms/step - accuracy: 0.9844 - loss: 0.0660 16/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9881 - loss: 0.0411  30/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9894 - loss: 0.0385 44/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9893 - loss: 0.0386 58/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9890 - loss: 0.0388 71/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9888 - loss: 0.0392 84/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9886 - loss: 0.0396 97/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9886 - loss: 0.0398110/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9885 - loss: 0.0399122/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9885 - loss: 0.0400135/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9885 - loss: 0.0401148/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9885 - loss: 0.0401161/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9885 - loss: 0.0401174/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9886 - loss: 0.0401188/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9886 - loss: 0.0400202/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9886 - loss: 0.0400216/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9886 - loss: 0.0400225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9886 - loss: 0.0400 - val_accuracy: 0.7525 - val_loss: 1.1077
Epoch 8/10
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 19ms/step - accuracy: 0.9844 - loss: 0.0338 15/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9892 - loss: 0.0267  29/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9905 - loss: 0.0254 43/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9908 - loss: 0.0256 57/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9911 - loss: 0.0255 71/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9914 - loss: 0.0251 85/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9915 - loss: 0.0248 99/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9915 - loss: 0.0248113/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9915 - loss: 0.0250127/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9915 - loss: 0.0251142/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9914 - loss: 0.0253157/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9914 - loss: 0.0255171/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9914 - loss: 0.0256185/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9914 - loss: 0.0257199/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9913 - loss: 0.0259213/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9913 - loss: 0.0260225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9913 - loss: 0.0261 - val_accuracy: 0.7550 - val_loss: 1.2353
Epoch 9/10
  1/225 ━━━━━━━━━━━━━━━━━━━━ 5s 24ms/step - accuracy: 1.0000 - loss: 0.0080 14/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9982 - loss: 0.0126  27/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9974 - loss: 0.0142 40/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9970 - loss: 0.0153 53/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9967 - loss: 0.0158 66/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9965 - loss: 0.0161 77/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9964 - loss: 0.0163 88/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9962 - loss: 0.0165 99/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9961 - loss: 0.0167112/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9960 - loss: 0.0169126/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9959 - loss: 0.0172140/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9957 - loss: 0.0176154/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9956 - loss: 0.0178167/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9955 - loss: 0.0181180/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9954 - loss: 0.0183194/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9953 - loss: 0.0186207/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9952 - loss: 0.0188221/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9951 - loss: 0.0190225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9950 - loss: 0.0190 - val_accuracy: 0.7469 - val_loss: 1.2951
Epoch 10/10
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9844 - loss: 0.0462 14/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9947 - loss: 0.0200  28/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9951 - loss: 0.0181 41/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9955 - loss: 0.0170 53/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9958 - loss: 0.0163 66/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9959 - loss: 0.0159 79/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9960 - loss: 0.0155 91/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9960 - loss: 0.0153103/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9959 - loss: 0.0152113/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9959 - loss: 0.0152123/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9958 - loss: 0.0152133/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9958 - loss: 0.0151145/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9958 - loss: 0.0151157/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9958 - loss: 0.0150170/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9958 - loss: 0.0150183/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9958 - loss: 0.0151196/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9957 - loss: 0.0151209/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9957 - loss: 0.0152222/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9957 - loss: 0.0152225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.9957 - loss: 0.0152 - val_accuracy: 0.7481 - val_loss: 1.3896
Accuracy (TF-IDF + Neural Network for sentiment analysis): 0.7393
  1/125 ━━━━━━━━━━━━━━━━━━━━ 3s 31ms/step 48/125 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step 105/125 ━━━━━━━━━━━━━━━━━━━━ 0s 977us/step125/125 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  

Classification Report:
              precision    recall  f1-score   support

           0       0.61      0.63      0.62      1355
           1       0.81      0.79      0.80      2645

    accuracy                           0.74      4000
   macro avg       0.71      0.71      0.71      4000
weighted avg       0.74      0.74      0.74      4000
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-8-output-3.png" width="721" height="566" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-8-output-4.png" width="1142" height="470" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_647939/3817474386.py:83: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x='Importance', y='Word', data=df_importance_nn_sentiment, palette='viridis')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-8-output-6.png" width="949" height="1143" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="comparison-of-bow-tf-idf-and-nn-models" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-bow-tf-idf-and-nn-models">Comparison of BoW, TF-IDF and NN Models</h3>
<p>We compared the accuracy of the Bag-of-Words, TF-IDF, and TF-IDF Neural Network models using a bar chart to visualize their performance side-by-side. TF-IDF showed a slight improvement in accuracy over Bag-of-Words, and the Neural Network performed similarly to TF-IDF with Logistic Regression.</p>
<div id="3f941d33" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare model accuracies in a bar plot</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>models_sentiment <span class="op">=</span> [<span class="st">'BOW + LogReg'</span>, <span class="st">'TF-IDF + LogReg'</span>, <span class="st">'TF-IDF + NN'</span>]</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>accuracies_sentiment <span class="op">=</span> [accuracy_bow_logistic, accuracy_tfidf_logistic, accuracy_nn_sentiment]</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame for plotting</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>df_metrics_sentiment <span class="op">=</span> pd.DataFrame({<span class="st">'Model'</span>: models_sentiment, <span class="st">'Accuracy'</span>: accuracies_sentiment})</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create bar chart</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.barplot(x<span class="op">=</span><span class="st">'Model'</span>, y<span class="op">=</span><span class="st">'Accuracy'</span>, data<span class="op">=</span>df_metrics_sentiment, palette<span class="op">=</span>[<span class="st">'#FF9671'</span>, <span class="st">'#FFC75F'</span>, <span class="st">'#845EC2'</span>])</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Add value labels on top of bars</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, acc <span class="kw">in</span> <span class="bu">enumerate</span>(accuracies_sentiment):</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    ax.text(i, acc <span class="op">+</span> <span class="fl">0.005</span>, <span class="ss">f'</span><span class="sc">{</span>acc<span class="sc">:.4f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Add labels and title</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Accuracy Comparison for Sentiment Analysis'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="fl">0.85</span>) <span class="co"># Adjust y-axis limit for better visualization</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Show plot</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_647939/2566848891.py:10: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  ax = sns.barplot(x='Model', y='Accuracy', data=df_metrics_sentiment, palette=['#FF9671', '#FFC75F', '#845EC2'])</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-9-output-2.png" width="950" height="567" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="distilbert-for-sentiment-analysis" class="level3">
<h3 class="anchored" data-anchor-id="distilbert-for-sentiment-analysis">DistilBERT for Sentiment Analysis</h3>
<p>The most advanced approach we tried was fine-tuning a DistilBERT model for our tasks. DistilBERT is a smaller, faster variant of the BERT transformer model that still retains much of BERT’s language understanding capabilities. It comes pre-trained on a large corpus and can be fine-tuned on a specific task with labeled data. We used Hugging Face’s Transformers library to load the distilbert-base-uncased model and added a classification layer on top. We then fine-tuned DistilBERT on our training set by feeding in the review text and training the model to predict the sentiment or rating. We expected transformer-based DistilBERT to outperform the simpler models because it can understand word order, context, and even semantics of the text. We used Talapas to fine-tune the DistilBERT model.</p>
<div id="4c649215" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set device for GPU if available, otherwise CPU</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load DistilBERT tokenizer and model for sequence classification</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>tokenizer_bert <span class="op">=</span> DistilBertTokenizer.from_pretrained(<span class="st">'distilbert-base-uncased'</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>model_bert <span class="op">=</span> DistilBertForSequenceClassification.from_pretrained(<span class="st">'distilbert-base-uncased'</span>, num_labels<span class="op">=</span><span class="dv">2</span>) <span class="co"># 2 labels: positive, negative</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>model_bert.to(device) <span class="co"># Move model to GPU if available</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataset class for DistilBERT</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SentimentDatasetBERT(Dataset):</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, texts, labels, tokenizer, max_len<span class="op">=</span><span class="dv">128</span>): <span class="co"># Adjusted max_len for BERT</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.texts <span class="op">=</span> texts</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> labels</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_len <span class="op">=</span> max_len</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.texts)</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="bu">str</span>(<span class="va">self</span>.texts.iloc[idx])</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="bu">int</span>(<span class="va">self</span>.labels.iloc[idx])</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>        encoding <span class="op">=</span> <span class="va">self</span>.tokenizer(</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>            text,</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>            truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="st">'max_length'</span>,</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>            max_length<span class="op">=</span><span class="va">self</span>.max_len,</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>            return_tensors<span class="op">=</span><span class="st">'pt'</span></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>            <span class="st">'input_ids'</span>: encoding[<span class="st">'input_ids'</span>].squeeze(<span class="dv">0</span>),</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>            <span class="st">'attention_mask'</span>: encoding[<span class="st">'attention_mask'</span>].squeeze(<span class="dv">0</span>),</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>            <span class="st">'labels'</span>: torch.tensor(label, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Create datasets and dataloaders for BERT</span></span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>train_dataset_bert <span class="op">=</span> SentimentDatasetBERT(X_train, y_train, tokenizer_bert)</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>test_dataset_bert <span class="op">=</span> SentimentDatasetBERT(X_test, y_test, tokenizer_bert)</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>train_loader_bert <span class="op">=</span> DataLoader(train_dataset_bert, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>) <span class="co"># Increased batch size for BERT</span></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>test_loader_bert <span class="op">=</span> DataLoader(test_dataset_bert, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer for BERT model</span></span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>optimizer_bert <span class="op">=</span> optim.AdamW(model_bert.parameters(), lr<span class="op">=</span><span class="fl">2e-5</span>)</span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>criterion_bert <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Training function for BERT model</span></span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model_bert(model, train_loader, optimizer, criterion, epochs<span class="op">=</span><span class="dv">3</span>, device<span class="op">=</span><span class="st">'cuda'</span>):</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>    model.to(device)  <span class="co"># Ensure model is on the correct device</span></span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> {<span class="st">'loss'</span>: [], <span class="st">'val_loss'</span>: [], <span class="st">'accuracy'</span>: [], <span class="st">'val_accuracy'</span>: []} <span class="co"># Training history</span></span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a>        correct_predictions <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>        total_samples <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a>        progress_bar <span class="op">=</span> tqdm(train_loader, desc<span class="op">=</span><span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss">"</span>, leave<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch <span class="kw">in</span> progress_bar:</span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a>            input_ids <span class="op">=</span> batch[<span class="st">'input_ids'</span>].to(device)</span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a>            attention_mask <span class="op">=</span> batch[<span class="st">'attention_mask'</span>].to(device)</span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> batch[<span class="st">'labels'</span>].to(device)</span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(input_ids, attention_mask<span class="op">=</span>attention_mask, labels<span class="op">=</span>labels)</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> outputs.loss</span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> outputs.logits</span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a>            total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a>            predictions <span class="op">=</span> torch.argmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a>            correct_predictions <span class="op">+=</span> (predictions <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a>            total_samples <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a>            progress_bar.set_postfix(loss<span class="op">=</span>total_loss <span class="op">/</span> <span class="bu">len</span>(progress_bar), acc<span class="op">=</span>correct_predictions<span class="op">/</span>total_samples)</span>
<span id="cb24-78"><a href="#cb24-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-79"><a href="#cb24-79" aria-hidden="true" tabindex="-1"></a>        avg_loss <span class="op">=</span> total_loss <span class="op">/</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb24-80"><a href="#cb24-80" aria-hidden="true" tabindex="-1"></a>        train_accuracy <span class="op">=</span> correct_predictions <span class="op">/</span> total_samples</span>
<span id="cb24-81"><a href="#cb24-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-82"><a href="#cb24-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validation step (optional, but good practice - omitted here for brevity but recommended for full projects)</span></span>
<span id="cb24-83"><a href="#cb24-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-84"><a href="#cb24-84" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> completed. Avg Loss: </span><span class="sc">{</span>avg_loss<span class="sc">:.4f}</span><span class="ss">, Train Accuracy: </span><span class="sc">{</span>train_accuracy<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb24-85"><a href="#cb24-85" aria-hidden="true" tabindex="-1"></a>        history[<span class="st">'loss'</span>].append(avg_loss)</span>
<span id="cb24-86"><a href="#cb24-86" aria-hidden="true" tabindex="-1"></a>        history[<span class="st">'accuracy'</span>].append(train_accuracy)</span>
<span id="cb24-87"><a href="#cb24-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-88"><a href="#cb24-88" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> history</span>
<span id="cb24-89"><a href="#cb24-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-90"><a href="#cb24-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate function for BERT model</span></span>
<span id="cb24-91"><a href="#cb24-91" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_model_bert(model, test_loader, device<span class="op">=</span><span class="st">'cuda'</span>):</span>
<span id="cb24-92"><a href="#cb24-92" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb24-93"><a href="#cb24-93" aria-hidden="true" tabindex="-1"></a>    model.to(device) <span class="co"># Ensure model is on the correct device</span></span>
<span id="cb24-94"><a href="#cb24-94" aria-hidden="true" tabindex="-1"></a>    y_pred, y_true <span class="op">=</span> [], []</span>
<span id="cb24-95"><a href="#cb24-95" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb24-96"><a href="#cb24-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch <span class="kw">in</span> test_loader:</span>
<span id="cb24-97"><a href="#cb24-97" aria-hidden="true" tabindex="-1"></a>            input_ids <span class="op">=</span> batch[<span class="st">'input_ids'</span>].to(device)</span>
<span id="cb24-98"><a href="#cb24-98" aria-hidden="true" tabindex="-1"></a>            attention_mask <span class="op">=</span> batch[<span class="st">'attention_mask'</span>].to(device)</span>
<span id="cb24-99"><a href="#cb24-99" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> batch[<span class="st">'labels'</span>].to(device)</span>
<span id="cb24-100"><a href="#cb24-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-101"><a href="#cb24-101" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(input_ids, attention_mask<span class="op">=</span>attention_mask, labels<span class="op">=</span>labels)</span>
<span id="cb24-102"><a href="#cb24-102" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> outputs.logits</span>
<span id="cb24-103"><a href="#cb24-103" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> torch.argmax(logits, dim<span class="op">=</span><span class="dv">1</span>).cpu().numpy()</span>
<span id="cb24-104"><a href="#cb24-104" aria-hidden="true" tabindex="-1"></a>            labels_cpu <span class="op">=</span> labels.cpu().numpy()</span>
<span id="cb24-105"><a href="#cb24-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-106"><a href="#cb24-106" aria-hidden="true" tabindex="-1"></a>            y_pred.extend(preds)</span>
<span id="cb24-107"><a href="#cb24-107" aria-hidden="true" tabindex="-1"></a>            y_true.extend(labels_cpu)</span>
<span id="cb24-108"><a href="#cb24-108" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y_true, y_pred</span>
<span id="cb24-109"><a href="#cb24-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-110"><a href="#cb24-110" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the DistilBERT model (adjust epochs as needed - 3-5 epochs is reasonable for demonstration)</span></span>
<span id="cb24-111"><a href="#cb24-111" aria-hidden="true" tabindex="-1"></a>bert_history <span class="op">=</span> train_model_bert(model_bert, train_loader_bert, optimizer_bert, criterion_bert, epochs<span class="op">=</span><span class="dv">3</span>, device<span class="op">=</span>device)</span>
<span id="cb24-112"><a href="#cb24-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-113"><a href="#cb24-113" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate DistilBERT model</span></span>
<span id="cb24-114"><a href="#cb24-114" aria-hidden="true" tabindex="-1"></a>y_true_bert, y_pred_bert <span class="op">=</span> evaluate_model_bert(model_bert, test_loader_bert, device<span class="op">=</span>device)</span>
<span id="cb24-115"><a href="#cb24-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-116"><a href="#cb24-116" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy and print classification report for BERT</span></span>
<span id="cb24-117"><a href="#cb24-117" aria-hidden="true" tabindex="-1"></a>accuracy_bert <span class="op">=</span> accuracy_score(y_true_bert, y_pred_bert)</span>
<span id="cb24-118"><a href="#cb24-118" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\\</span><span class="ss">nAccuracy (DistilBERT for sentiment analysis): </span><span class="sc">{</span>accuracy_bert<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb24-119"><a href="#cb24-119" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\\</span><span class="st">nClassification Report (DistilBERT):"</span>)</span>
<span id="cb24-120"><a href="#cb24-120" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_true_bert, y_pred_bert))</span>
<span id="cb24-121"><a href="#cb24-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-122"><a href="#cb24-122" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion Matrix for DistilBERT</span></span>
<span id="cb24-123"><a href="#cb24-123" aria-hidden="true" tabindex="-1"></a>cm_bert <span class="op">=</span> confusion_matrix(y_true_bert, y_pred_bert)</span>
<span id="cb24-124"><a href="#cb24-124" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb24-125"><a href="#cb24-125" aria-hidden="true" tabindex="-1"></a>sns.heatmap(cm_bert, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'d'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>,</span>
<span id="cb24-126"><a href="#cb24-126" aria-hidden="true" tabindex="-1"></a>            xticklabels<span class="op">=</span>[<span class="st">'Negative'</span>, <span class="st">'Positive'</span>],</span>
<span id="cb24-127"><a href="#cb24-127" aria-hidden="true" tabindex="-1"></a>            yticklabels<span class="op">=</span>[<span class="st">'Negative'</span>, <span class="st">'Positive'</span>])</span>
<span id="cb24-128"><a href="#cb24-128" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted'</span>)</span>
<span id="cb24-129"><a href="#cb24-129" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True'</span>)</span>
<span id="cb24-130"><a href="#cb24-130" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix: DistilBERT for Sentiment Analysis'</span>)</span>
<span id="cb24-131"><a href="#cb24-131" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb24-132"><a href="#cb24-132" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb24-133"><a href="#cb24-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-134"><a href="#cb24-134" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare all model accuracies in a bar plot including BERT</span></span>
<span id="cb24-135"><a href="#cb24-135" aria-hidden="true" tabindex="-1"></a>models_all_sentiment <span class="op">=</span> [<span class="st">'BOW + LogReg'</span>, <span class="st">'TF-IDF + LogReg'</span>, <span class="st">'DistilBERT'</span>]</span>
<span id="cb24-136"><a href="#cb24-136" aria-hidden="true" tabindex="-1"></a>accuracies_all_sentiment <span class="op">=</span> [accuracy_bow_logistic, accuracy_tfidf_logistic, accuracy_bert]</span>
<span id="cb24-137"><a href="#cb24-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-138"><a href="#cb24-138" aria-hidden="true" tabindex="-1"></a>df_metrics_all_sentiment <span class="op">=</span> pd.DataFrame({<span class="st">'Model'</span>: models_all_sentiment, <span class="st">'Accuracy'</span>: accuracies_all_sentiment})</span>
<span id="cb24-139"><a href="#cb24-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-140"><a href="#cb24-140" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb24-141"><a href="#cb24-141" aria-hidden="true" tabindex="-1"></a>ax_all_sent <span class="op">=</span> sns.barplot(x<span class="op">=</span><span class="st">'Model'</span>, y<span class="op">=</span><span class="st">'Accuracy'</span>, data<span class="op">=</span>df_metrics_all_sentiment, palette<span class="op">=</span>[<span class="st">'#FF9671'</span>, <span class="st">'#FFC75F'</span>, <span class="st">'#845EC2'</span>])</span>
<span id="cb24-142"><a href="#cb24-142" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, acc <span class="kw">in</span> <span class="bu">enumerate</span>(accuracies_all_sentiment):</span>
<span id="cb24-143"><a href="#cb24-143" aria-hidden="true" tabindex="-1"></a>    ax_all_sent.text(i, acc <span class="op">+</span> <span class="fl">0.005</span>, <span class="ss">f'</span><span class="sc">{</span>acc<span class="sc">:.4f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb24-144"><a href="#cb24-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-145"><a href="#cb24-145" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Accuracy Comparison for Sentiment Analysis (including DistilBERT)'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb24-146"><a href="#cb24-146" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb24-147"><a href="#cb24-147" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="fl">0.9</span>) <span class="co"># Adjust y-axis limit</span></span>
<span id="cb24-148"><a href="#cb24-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-149"><a href="#cb24-149" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb24-150"><a href="#cb24-150" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>DistilBERT ended up achieving a score of <strong>80.40% accuracy</strong>.</p>
<p>Here is the confusion matrix produced:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./BERT_Sentiment.png" class="img-fluid figure-img"></p>
<figcaption>BERT_Sentiment_Confusion</figcaption>
</figure>
</div>
<p>DistilBERT achieved the highest accuracy in sentiment classification, outperforming both Bag-of-Words and TF-IDF models with Logistic Regression. This improvement comes at the cost of increased computational complexity and training time, but demonstrates the potential benefits of using transformer-based models for sentiment analysis on book reviews.</p>
</section>
</section>
<section id="multiclass-classification" class="level2">
<h2 class="anchored" data-anchor-id="multiclass-classification">Multiclass Classification</h2>
<p>Our next task was to predict the exact star rating (1 to 5) of a review directly. We treated this as a 5-class classification task. We considered a regression approach to predict a continuous rating, but since ratings in the dataset are discrete, classification with discrete classes was more straightforward for evaluation. This task is substantially harder than binary sentiment, the model must discern fine differences, not just positive vs negative. We started with using a multinomial logistic regression to predict one of the five rating classes. We again started with Bag-of-Words and TF-IDF with Logistic Regression, and then explored a feed-forward Neural Network model for multiclass classification and also fine tuned DistilBER.</p>
<section id="bag-of-words-and-tf-idf-with-logistic-regression-for-rating-prediction" class="level3">
<h3 class="anchored" data-anchor-id="bag-of-words-and-tf-idf-with-logistic-regression-for-rating-prediction">Bag of Words and TF-IDF with Logistic Regression for Rating Prediction</h3>
<p>We extended our use of Bag-of-Words and TF-IDF to predict ratings on a 1-to-5 scale. Logistic Regression was adapted for multiclass classification using the ‘multinomial’ setting.</p>
<section id="bag-of-words-for-rating-prediction" class="level4">
<h4 class="anchored" data-anchor-id="bag-of-words-for-rating-prediction">Bag of Words for Rating Prediction</h4>
<div id="6ab1d017" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data for rating prediction (using original ratings 1-5)</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>X_train_rating, X_test_rating, y_train_rating, y_test_rating <span class="op">=</span> train_test_split(</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'review_text'</span>],</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'rating'</span>], <span class="co"># Use original ratings (1-5)</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Pipeline with CountVectorizer and Logistic Regression for multiclass classification</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>pipeline_bow_rating <span class="op">=</span> Pipeline([</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'vectorizer'</span>, CountVectorizer(max_features<span class="op">=</span><span class="dv">5000</span>, min_df<span class="op">=</span><span class="dv">5</span>, stop_words<span class="op">=</span><span class="st">'english'</span>)),</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'classifier'</span>, LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>, C<span class="op">=</span><span class="fl">1.0</span>, multi_class<span class="op">=</span><span class="st">'multinomial'</span>)) <span class="co"># Specify multi_class='multinomial'</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>pipeline_bow_rating.fit(X_train_rating, y_train_rating)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>y_pred_rating_bow <span class="op">=</span> pipeline_bow_rating.predict(X_test_rating)</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>accuracy_bow_rating_logistic <span class="op">=</span> accuracy_score(y_test_rating, y_pred_rating_bow)</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy (Bag of Words + Logistic Regression for ratings): </span><span class="sc">{</span>accuracy_bow_rating_logistic<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification Report:"</span>)</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test_rating, y_pred_rating_bow))</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion Matrix</span></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>cm_rating_bow <span class="op">=</span> confusion_matrix(y_test_rating, y_pred_rating_bow)</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>sns.heatmap(cm_rating_bow, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'d'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>,</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>            xticklabels<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>],</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>            yticklabels<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted Rating'</span>)</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True Rating'</span>)</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix: BoW + Logistic Regression for Ratings'</span>)</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature Importance for each rating class (example for ratings 1-5)</span></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>vectorizer_rating_bow <span class="op">=</span> pipeline_bow_rating.named_steps[<span class="st">'vectorizer'</span>]</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>classifier_rating_bow <span class="op">=</span> pipeline_bow_rating.named_steps[<span class="st">'classifier'</span>]</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>feature_names_rating_bow <span class="op">=</span> vectorizer_rating_bow.get_feature_names_out()</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> rating_class <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>):</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Coefficients for each class (ratings 1 to 5)</span></span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">hasattr</span>(classifier_rating_bow, <span class="st">'coef_'</span>) <span class="kw">and</span> classifier_rating_bow.coef_.shape[<span class="dv">0</span>] <span class="op">&gt;=</span> rating_class:</span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>        coefs_rating <span class="op">=</span> classifier_rating_bow.coef_[rating_class<span class="op">-</span><span class="dv">1</span>] <span class="co"># Adjust index for 0-based indexing</span></span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Top words for each rating</span></span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>        top_words_rating <span class="op">=</span> pd.DataFrame({</span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Word'</span>: [feature_names_rating_bow[idx] <span class="cf">for</span> idx <span class="kw">in</span> coefs_rating.argsort()[<span class="op">-</span><span class="dv">15</span>:]][::<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Coefficient'</span>: <span class="bu">sorted</span>(coefs_rating)[<span class="op">-</span><span class="dv">15</span>:][::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Top words associated with rating </span><span class="sc">{</span>rating_class<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>        display(top_words_rating)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/home/default/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy (Bag of Words + Logistic Regression for ratings): 0.4597

Classification Report:
              precision    recall  f1-score   support

           1       0.46      0.37      0.41       138
           2       0.32      0.27      0.30       339
           3       0.38      0.37      0.37       878
           4       0.46      0.50      0.48      1441
           5       0.55      0.55      0.55      1204

    accuracy                           0.46      4000
   macro avg       0.43      0.41      0.42      4000
weighted avg       0.46      0.46      0.46      4000
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-11-output-3.png" width="885" height="758" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top words associated with rating 1:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>waste</td>
<td>2.185561</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>dnf</td>
<td>1.952132</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>worst</td>
<td>1.511711</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>boring</td>
<td>1.471345</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>conversations</td>
<td>1.377387</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>awful</td>
<td>1.257038</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>potential</td>
<td>1.147633</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>disappointment</td>
<td>1.069420</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>flat</td>
<td>1.060459</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>cliche</td>
<td>1.033409</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>ridiculous</td>
<td>1.016144</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>pretentious</td>
<td>1.007098</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>goodreads</td>
<td>0.988973</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>largely</td>
<td>0.979036</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>quinn</td>
<td>0.965561</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top words associated with rating 2:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>disappointing</td>
<td>1.565330</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>meh</td>
<td>1.459669</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>execution</td>
<td>1.296778</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>obnoxious</td>
<td>1.294076</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>hearing</td>
<td>1.236577</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>connecting</td>
<td>1.213847</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>jarring</td>
<td>1.202940</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>patience</td>
<td>1.189149</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>dropped</td>
<td>1.185928</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>filler</td>
<td>1.166428</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>challenge</td>
<td>1.141155</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>weak</td>
<td>1.140272</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>mildly</td>
<td>1.107250</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>cup</td>
<td>1.101199</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>despair</td>
<td>1.100111</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top words associated with rating 3:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>pushes</td>
<td>1.579605</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>kira</td>
<td>1.493114</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>fence</td>
<td>1.373182</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>meh</td>
<td>1.273955</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>sequels</td>
<td>1.266506</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>improvement</td>
<td>1.244827</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>sight</td>
<td>1.239336</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>hurting</td>
<td>1.223412</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>desires</td>
<td>1.220363</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>quarter</td>
<td>1.197817</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>redemption</td>
<td>1.141973</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>encounter</td>
<td>1.116555</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>generous</td>
<td>1.105495</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>oldest</td>
<td>1.103525</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>switched</td>
<td>1.097709</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top words associated with rating 4:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>rude</td>
<td>1.582930</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>tall</td>
<td>1.461034</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>singing</td>
<td>1.398066</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>improved</td>
<td>1.242753</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>rival</td>
<td>1.229282</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>holds</td>
<td>1.199828</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>hmmm</td>
<td>1.192374</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>sticking</td>
<td>1.167500</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>overlook</td>
<td>1.165035</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>lingering</td>
<td>1.159455</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>ideal</td>
<td>1.137475</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>mirror</td>
<td>1.132498</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>translation</td>
<td>1.105592</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>gritty</td>
<td>1.105330</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>complain</td>
<td>1.100009</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top words associated with rating 5:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>assured</td>
<td>1.553455</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>masterful</td>
<td>1.413564</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>delivers</td>
<td>1.399301</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>penny</td>
<td>1.348632</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>cried</td>
<td>1.343608</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>rely</td>
<td>1.326905</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>shoulder</td>
<td>1.306346</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>brilliantly</td>
<td>1.261499</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>masterpiece</td>
<td>1.243753</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>wondered</td>
<td>1.240454</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>tears</td>
<td>1.218231</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>sending</td>
<td>1.211100</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>highly</td>
<td>1.164070</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>relatively</td>
<td>1.163111</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>immensely</td>
<td>1.162450</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="tf-idf-for-rating-prediction" class="level4">
<h4 class="anchored" data-anchor-id="tf-idf-for-rating-prediction">TF-IDF for Rating Prediction</h4>
<div id="0d493979" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pipeline with TF-IDF and Logistic Regression for multiclass classification</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>pipeline_tfidf_rating <span class="op">=</span> Pipeline([</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'vectorizer'</span>, TfidfVectorizer(max_features<span class="op">=</span><span class="dv">5000</span>, min_df<span class="op">=</span><span class="dv">5</span>, stop_words<span class="op">=</span><span class="st">'english'</span>)),</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'classifier'</span>, LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>, C<span class="op">=</span><span class="fl">1.0</span>, multi_class<span class="op">=</span><span class="st">'multinomial'</span>))</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the TF-IDF model for rating prediction</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>pipeline_tfidf_rating.fit(X_train_rating, y_train_rating)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>y_pred_rating_tfidf <span class="op">=</span> pipeline_tfidf_rating.predict(X_test_rating)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>accuracy_tfidf_rating_logistic <span class="op">=</span> accuracy_score(y_test_rating, y_pred_rating_tfidf)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy (TF-IDF + Logistic Regression for ratings): </span><span class="sc">{</span>accuracy_tfidf_rating_logistic<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification Report:"</span>)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test_rating, y_pred_rating_tfidf))</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion Matrix</span></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>cm_rating_tfidf <span class="op">=</span> confusion_matrix(y_test_rating, y_pred_rating_tfidf)</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>sns.heatmap(cm_rating_tfidf, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'d'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>,</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>            xticklabels<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>],</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>            yticklabels<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted Rating'</span>)</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True Rating'</span>)</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix: TF-IDF + Logistic Regression for Ratings'</span>)</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature Importance for each rating class (example for ratings 1-5)</span></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>vectorizer_rating_tfidf <span class="op">=</span> pipeline_tfidf_rating.named_steps[<span class="st">'vectorizer'</span>]</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>classifier_rating_tfidf <span class="op">=</span> pipeline_tfidf_rating.named_steps[<span class="st">'classifier'</span>]</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>feature_names_rating_tfidf <span class="op">=</span> vectorizer_rating_tfidf.get_feature_names_out()</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> rating_class <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>):</span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">hasattr</span>(classifier_rating_tfidf, <span class="st">'coef_'</span>) <span class="kw">and</span> classifier_rating_tfidf.coef_.shape[<span class="dv">0</span>] <span class="op">&gt;=</span> rating_class:</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>        coefs_rating <span class="op">=</span> classifier_rating_tfidf.coef_[rating_class<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>        top_words_rating <span class="op">=</span> pd.DataFrame({</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Word'</span>: [feature_names_rating_tfidf[idx] <span class="cf">for</span> idx <span class="kw">in</span> coefs_rating.argsort()[<span class="op">-</span><span class="dv">15</span>:]][::<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Coefficient'</span>: <span class="bu">sorted</span>(coefs_rating)[<span class="op">-</span><span class="dv">15</span>:][::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Top words associated with rating </span><span class="sc">{</span>rating_class<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a>        display(top_words_rating)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/home/default/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy (TF-IDF + Logistic Regression for ratings): 0.4930

Classification Report:
              precision    recall  f1-score   support

           1       0.68      0.11      0.19       138
           2       0.36      0.17      0.23       339
           3       0.42      0.36      0.39       878
           4       0.47      0.59      0.52      1441
           5       0.58      0.61      0.59      1204

    accuracy                           0.49      4000
   macro avg       0.50      0.37      0.38      4000
weighted avg       0.49      0.49      0.48      4000
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-12-output-3.png" width="885" height="758" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top words associated with rating 1:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>dnf</td>
<td>3.871140</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>waste</td>
<td>2.970713</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>boring</td>
<td>2.875207</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>worst</td>
<td>2.474526</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>finish</td>
<td>2.237463</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>awful</td>
<td>2.164222</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>just</td>
<td>2.111561</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>pages</td>
<td>2.064920</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>sorry</td>
<td>1.861956</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>tried</td>
<td>1.850658</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>couldn</td>
<td>1.834866</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>main</td>
<td>1.682543</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>annoying</td>
<td>1.665279</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>didn</td>
<td>1.643840</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>ridiculous</td>
<td>1.613827</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top words associated with rating 2:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>disappointing</td>
<td>2.864996</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>didn</td>
<td>2.383351</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>disappointed</td>
<td>2.258781</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>boring</td>
<td>2.254321</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>unfortunately</td>
<td>2.209502</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>bored</td>
<td>2.198359</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>meh</td>
<td>2.136748</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>wasn</td>
<td>2.100340</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>premise</td>
<td>2.004330</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>weak</td>
<td>1.935709</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>just</td>
<td>1.881026</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>felt</td>
<td>1.785008</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>dull</td>
<td>1.743593</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>care</td>
<td>1.672492</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>supposed</td>
<td>1.599106</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top words associated with rating 3:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>liked</td>
<td>2.599303</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>decent</td>
<td>1.950189</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>enjoyable</td>
<td>1.837791</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>interesting</td>
<td>1.833545</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>mixed</td>
<td>1.784414</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>cute</td>
<td>1.725586</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>ok</td>
<td>1.703822</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>meh</td>
<td>1.606525</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>stars</td>
<td>1.605503</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>wasn</td>
<td>1.597668</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>nice</td>
<td>1.591716</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>overall</td>
<td>1.566002</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>quick</td>
<td>1.525002</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>lacking</td>
<td>1.513949</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>sequels</td>
<td>1.504849</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top words associated with rating 4:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>enjoyed</td>
<td>2.813995</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>fun</td>
<td>1.760797</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>overall</td>
<td>1.721080</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>wait</td>
<td>1.695142</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>liked</td>
<td>1.641830</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>bit</td>
<td>1.526375</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>definitely</td>
<td>1.485034</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>little</td>
<td>1.472027</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>loved</td>
<td>1.457402</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>great</td>
<td>1.445003</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>cast</td>
<td>1.430638</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>25</td>
<td>1.346928</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>attitude</td>
<td>1.330633</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>focus</td>
<td>1.324933</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>captivating</td>
<td>1.318663</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top words associated with rating 5:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Word</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>loved</td>
<td>5.327090</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>amazing</td>
<td>3.639897</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>love</td>
<td>3.294756</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>highly</td>
<td>2.925506</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>awesome</td>
<td>2.781752</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>perfect</td>
<td>2.757990</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>heart</td>
<td>2.717939</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>great</td>
<td>2.668084</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>fantastic</td>
<td>2.601819</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>wait</td>
<td>2.517390</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>beautiful</td>
<td>2.511246</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>brilliant</td>
<td>2.464146</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>absolutely</td>
<td>2.377688</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>excellent</td>
<td>2.259023</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>favorite</td>
<td>2.196589</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
</section>
<section id="tf-idf-with-simple-neural-network-for-rating-prediction" class="level3">
<h3 class="anchored" data-anchor-id="tf-idf-with-simple-neural-network-for-rating-prediction">TF-IDF with Simple Neural Network for Rating Prediction</h3>
<p>To explore a more complex model, we implemented a simple Neural Network (NN) using TensorFlow/Keras for multiclass rating prediction. We extended the neural network approach to the 5-class scenario by using a final layer with 5 neurons. The model outputs a probability distribution across the 5 rating classes for each review. We used TF-IDF features as input to this network and compared its performance against Logistic Regression.</p>
<div id="ac8acd5b" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert ratings to categorical for NN</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>y_train_categorical <span class="op">=</span> to_categorical(y_train_rating <span class="op">-</span> <span class="dv">1</span>, num_classes<span class="op">=</span><span class="dv">5</span>) <span class="co"># -1 to make ratings 0-indexed</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>y_test_categorical <span class="op">=</span> to_categorical(y_test_rating <span class="op">-</span> <span class="dv">1</span>, num_classes<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="co"># TF-IDF vectorization for Neural Network input</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>vectorizer_nn <span class="op">=</span> TfidfVectorizer(max_features<span class="op">=</span><span class="dv">5000</span>, min_df<span class="op">=</span><span class="dv">5</span>, stop_words<span class="op">=</span><span class="st">'english'</span>)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>X_train_tfidf_nn <span class="op">=</span> vectorizer_nn.fit_transform(X_train_rating).toarray()</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>X_test_tfidf_nn <span class="op">=</span> vectorizer_nn.transform(X_test_rating).toarray()</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>input_dim_nn <span class="op">=</span> X_train_tfidf_nn.shape[<span class="dv">1</span>]</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Neural Network model</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>model_nn <span class="op">=</span> Sequential([</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>, input_shape<span class="op">=</span>(input_dim_nn,)),</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>    Dropout(<span class="fl">0.4</span>),</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>    Dropout(<span class="fl">0.3</span>),</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>    Dropout(<span class="fl">0.2</span>),</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">5</span>, activation<span class="op">=</span><span class="st">'softmax'</span>) <span class="co"># 5 classes for ratings 1-5</span></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>model_nn.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>,</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>history_nn <span class="op">=</span> model_nn.fit(X_train_tfidf_nn, y_train_categorical,</span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a>                    epochs<span class="op">=</span><span class="dv">15</span>, batch_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>                    validation_split<span class="op">=</span><span class="fl">0.1</span>, verbose<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>_, accuracy_nn <span class="op">=</span> model_nn.evaluate(X_test_tfidf_nn, y_test_categorical, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy (TF-IDF + Neural Network for ratings): </span><span class="sc">{</span>accuracy_nn<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions and Classification Report</span></span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>y_pred_prob_nn <span class="op">=</span> model_nn.predict(X_test_tfidf_nn)</span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>y_pred_classes_nn <span class="op">=</span> np.argmax(y_pred_prob_nn, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a>y_pred_nn <span class="op">=</span> y_pred_classes_nn <span class="op">+</span> <span class="dv">1</span> <span class="co"># Convert back to original rating scale (1-5)</span></span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification Report:"</span>)</span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test_rating, y_pred_nn))</span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion Matrix for Neural Network</span></span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a>cm_nn <span class="op">=</span> confusion_matrix(y_test_rating, y_pred_nn)</span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a>sns.heatmap(cm_nn, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'d'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>,</span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a>            xticklabels<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>],</span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a>            yticklabels<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted Rating'</span>)</span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True Rating'</span>)</span>
<span id="cb41-51"><a href="#cb41-51" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix: TF-IDF + Neural Network for Ratings'</span>)</span>
<span id="cb41-52"><a href="#cb41-52" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb41-53"><a href="#cb41-53" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb41-54"><a href="#cb41-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-55"><a href="#cb41-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training history</span></span>
<span id="cb41-56"><a href="#cb41-56" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb41-57"><a href="#cb41-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-58"><a href="#cb41-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot accuracy</span></span>
<span id="cb41-59"><a href="#cb41-59" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb41-60"><a href="#cb41-60" aria-hidden="true" tabindex="-1"></a>plt.plot(history_nn.history[<span class="st">'accuracy'</span>], label<span class="op">=</span><span class="st">'Train Accuracy'</span>)</span>
<span id="cb41-61"><a href="#cb41-61" aria-hidden="true" tabindex="-1"></a>plt.plot(history_nn.history[<span class="st">'val_accuracy'</span>], label<span class="op">=</span><span class="st">'Validation Accuracy'</span>)</span>
<span id="cb41-62"><a href="#cb41-62" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Accuracy'</span>)</span>
<span id="cb41-63"><a href="#cb41-63" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb41-64"><a href="#cb41-64" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb41-65"><a href="#cb41-65" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb41-66"><a href="#cb41-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-67"><a href="#cb41-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot loss</span></span>
<span id="cb41-68"><a href="#cb41-68" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb41-69"><a href="#cb41-69" aria-hidden="true" tabindex="-1"></a>plt.plot(history_nn.history[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'Train Loss'</span>)</span>
<span id="cb41-70"><a href="#cb41-70" aria-hidden="true" tabindex="-1"></a>plt.plot(history_nn.history[<span class="st">'val_loss'</span>], label<span class="op">=</span><span class="st">'Validation Loss'</span>)</span>
<span id="cb41-71"><a href="#cb41-71" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Loss'</span>)</span>
<span id="cb41-72"><a href="#cb41-72" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb41-73"><a href="#cb41-73" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb41-74"><a href="#cb41-74" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb41-75"><a href="#cb41-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-76"><a href="#cb41-76" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb41-77"><a href="#cb41-77" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb41-78"><a href="#cb41-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-79"><a href="#cb41-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature Importance - using weights from the first layer of the NN</span></span>
<span id="cb41-80"><a href="#cb41-80" aria-hidden="true" tabindex="-1"></a><span class="co"># (Note: Feature importance in NNs is complex and this is a simplified approach)</span></span>
<span id="cb41-81"><a href="#cb41-81" aria-hidden="true" tabindex="-1"></a>weights_nn <span class="op">=</span> model_nn.layers[<span class="dv">0</span>].get_weights()[<span class="dv">0</span>] <span class="co"># Weights of the first Dense layer</span></span>
<span id="cb41-82"><a href="#cb41-82" aria-hidden="true" tabindex="-1"></a>feature_importance_nn <span class="op">=</span> np.mean(np.<span class="bu">abs</span>(weights_nn), axis<span class="op">=</span><span class="dv">1</span>) <span class="co"># Mean absolute weight for each feature</span></span>
<span id="cb41-83"><a href="#cb41-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-84"><a href="#cb41-84" aria-hidden="true" tabindex="-1"></a>top_indices_nn <span class="op">=</span> feature_importance_nn.argsort()[<span class="op">-</span><span class="dv">30</span>:][::<span class="op">-</span><span class="dv">1</span>] <span class="co"># Top 30 features</span></span>
<span id="cb41-85"><a href="#cb41-85" aria-hidden="true" tabindex="-1"></a>df_importance_nn <span class="op">=</span> pd.DataFrame({</span>
<span id="cb41-86"><a href="#cb41-86" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Word'</span>: [vectorizer_nn.get_feature_names_out()[idx] <span class="cf">for</span> idx <span class="kw">in</span> top_indices_nn],</span>
<span id="cb41-87"><a href="#cb41-87" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Importance'</span>: feature_importance_nn[top_indices_nn]</span>
<span id="cb41-88"><a href="#cb41-88" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb41-89"><a href="#cb41-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-90"><a href="#cb41-90" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">12</span>))</span>
<span id="cb41-91"><a href="#cb41-91" aria-hidden="true" tabindex="-1"></a>sns.barplot(x<span class="op">=</span><span class="st">'Importance'</span>, y<span class="op">=</span><span class="st">'Word'</span>, data<span class="op">=</span>df_importance_nn, palette<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb41-92"><a href="#cb41-92" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Neural Network Feature Importance'</span>, fontsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb41-93"><a href="#cb41-93" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Mean Absolute Weight'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb41-94"><a href="#cb41-94" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb41-95"><a href="#cb41-95" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/home/default/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 2:46 741ms/step - accuracy: 0.2500 - loss: 1.6053 15/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3169 - loss: 1.5889     28/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3260 - loss: 1.5686 41/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3292 - loss: 1.5476 53/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3308 - loss: 1.5310 65/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3315 - loss: 1.5175 76/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3320 - loss: 1.5072 87/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3328 - loss: 1.4985 99/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3340 - loss: 1.4900112/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3358 - loss: 1.4809125/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3380 - loss: 1.4720137/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3403 - loss: 1.4641149/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3427 - loss: 1.4565161/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3452 - loss: 1.4492173/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3477 - loss: 1.4422185/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3501 - loss: 1.4356197/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3525 - loss: 1.4293209/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3548 - loss: 1.4233222/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.3572 - loss: 1.4170225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.3579 - loss: 1.4151 - val_accuracy: 0.4994 - val_loss: 1.1488
Epoch 2/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.5781 - loss: 0.9915 16/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.4913 - loss: 1.1195  31/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.4948 - loss: 1.1093 42/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5001 - loss: 1.1023 54/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5027 - loss: 1.0982 67/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5053 - loss: 1.0947 80/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5078 - loss: 1.0916 94/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5100 - loss: 1.0886108/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5118 - loss: 1.0864122/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5136 - loss: 1.0841136/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5151 - loss: 1.0823151/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5163 - loss: 1.0807165/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5170 - loss: 1.0796179/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5178 - loss: 1.0786193/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5185 - loss: 1.0778207/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5191 - loss: 1.0770221/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5198 - loss: 1.0763225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.5200 - loss: 1.0761 - val_accuracy: 0.4950 - val_loss: 1.1218
Epoch 3/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.5781 - loss: 1.0655 14/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6238 - loss: 0.9198  26/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6318 - loss: 0.8974 39/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6339 - loss: 0.8904 50/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6343 - loss: 0.8877 62/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6341 - loss: 0.8861 74/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6335 - loss: 0.8860 88/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6321 - loss: 0.8873102/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6306 - loss: 0.8889117/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6293 - loss: 0.8901131/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6283 - loss: 0.8913146/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6274 - loss: 0.8927160/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6267 - loss: 0.8938174/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6259 - loss: 0.8949188/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6251 - loss: 0.8960202/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6245 - loss: 0.8972216/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6237 - loss: 0.8984225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.6233 - loss: 0.8992 - val_accuracy: 0.4900 - val_loss: 1.1685
Epoch 4/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 19ms/step - accuracy: 0.6719 - loss: 0.7766 17/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7114 - loss: 0.7638  32/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7060 - loss: 0.7651 46/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7027 - loss: 0.7630 61/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7007 - loss: 0.7615 75/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6994 - loss: 0.7602 88/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6990 - loss: 0.7588102/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6988 - loss: 0.7570117/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6984 - loss: 0.7562131/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6979 - loss: 0.7558144/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6975 - loss: 0.7558157/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6971 - loss: 0.7561170/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6967 - loss: 0.7565183/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6962 - loss: 0.7571196/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6957 - loss: 0.7579208/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6952 - loss: 0.7587221/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.6946 - loss: 0.7594225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.6945 - loss: 0.7597 - val_accuracy: 0.4737 - val_loss: 1.2967
Epoch 5/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 19ms/step - accuracy: 0.7188 - loss: 0.7231 16/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7550 - loss: 0.6366  30/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7608 - loss: 0.6233 44/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7608 - loss: 0.6204 58/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7608 - loss: 0.6163 71/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7604 - loss: 0.6143 85/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7605 - loss: 0.6130 98/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7608 - loss: 0.6121111/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7609 - loss: 0.6118125/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7610 - loss: 0.6115139/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7613 - loss: 0.6110152/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7615 - loss: 0.6106166/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7617 - loss: 0.6102180/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7619 - loss: 0.6098193/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7619 - loss: 0.6100207/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7618 - loss: 0.6101221/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7618 - loss: 0.6104225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.7618 - loss: 0.6106 - val_accuracy: 0.4663 - val_loss: 1.4623
Epoch 6/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.8906 - loss: 0.4355 17/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8668 - loss: 0.4417  30/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8637 - loss: 0.4396 43/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8601 - loss: 0.4379 56/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8577 - loss: 0.4377 69/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8560 - loss: 0.4369 81/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8540 - loss: 0.4379 93/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8523 - loss: 0.4387106/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8507 - loss: 0.4392118/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8493 - loss: 0.4400130/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8481 - loss: 0.4411143/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8468 - loss: 0.4420156/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8456 - loss: 0.4434170/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8444 - loss: 0.4449184/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8433 - loss: 0.4463197/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8425 - loss: 0.4477211/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8416 - loss: 0.4493225/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8409 - loss: 0.4508225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.8408 - loss: 0.4509 - val_accuracy: 0.4625 - val_loss: 1.7131
Epoch 7/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9219 - loss: 0.3129 16/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8984 - loss: 0.3461  30/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8891 - loss: 0.3529 44/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8852 - loss: 0.3516 58/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8845 - loss: 0.3487 72/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8840 - loss: 0.3470 85/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8835 - loss: 0.3464 98/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8828 - loss: 0.3470111/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8821 - loss: 0.3475125/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8814 - loss: 0.3480139/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8809 - loss: 0.3484153/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8803 - loss: 0.3488166/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8800 - loss: 0.3492179/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8797 - loss: 0.3495192/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8794 - loss: 0.3500205/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8790 - loss: 0.3506218/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8787 - loss: 0.3512225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.8785 - loss: 0.3516 - val_accuracy: 0.4494 - val_loss: 1.8914
Epoch 8/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9531 - loss: 0.2357 14/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9285 - loss: 0.2442  28/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9218 - loss: 0.2490 41/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9173 - loss: 0.2551 55/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9137 - loss: 0.2604 69/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9109 - loss: 0.2648 83/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9092 - loss: 0.2673 96/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9081 - loss: 0.2690110/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9072 - loss: 0.2706123/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9066 - loss: 0.2720137/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9060 - loss: 0.2731151/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9056 - loss: 0.2739164/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9054 - loss: 0.2744177/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9051 - loss: 0.2751190/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9048 - loss: 0.2758204/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9045 - loss: 0.2765218/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9042 - loss: 0.2772225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9041 - loss: 0.2776 - val_accuracy: 0.4481 - val_loss: 2.1770
Epoch 9/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9688 - loss: 0.1712 15/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9285 - loss: 0.2014  28/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9276 - loss: 0.2118 41/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9277 - loss: 0.2156 53/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9272 - loss: 0.2180 65/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9267 - loss: 0.2195 77/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9260 - loss: 0.2211 89/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9254 - loss: 0.2220100/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9251 - loss: 0.2228112/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9248 - loss: 0.2235124/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9247 - loss: 0.2242136/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9245 - loss: 0.2244148/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9244 - loss: 0.2246161/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9243 - loss: 0.2246174/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9242 - loss: 0.2245187/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9239 - loss: 0.2248200/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9236 - loss: 0.2253214/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9233 - loss: 0.2258225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9231 - loss: 0.2262 - val_accuracy: 0.4525 - val_loss: 2.3150
Epoch 10/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 18ms/step - accuracy: 0.9844 - loss: 0.0780 14/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9416 - loss: 0.1848  27/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9371 - loss: 0.1933 39/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9350 - loss: 0.1990 52/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9341 - loss: 0.2016 64/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9341 - loss: 0.2016 77/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9341 - loss: 0.2013 90/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9339 - loss: 0.2013103/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9337 - loss: 0.2012117/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9335 - loss: 0.2013131/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9334 - loss: 0.2012144/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9333 - loss: 0.2010157/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9334 - loss: 0.2005170/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9335 - loss: 0.2001182/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9335 - loss: 0.1998194/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9335 - loss: 0.1996206/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9335 - loss: 0.1995218/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9334 - loss: 0.1994225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9333 - loss: 0.1994 - val_accuracy: 0.4506 - val_loss: 2.5922
Epoch 11/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 5s 23ms/step - accuracy: 0.9844 - loss: 0.0703 14/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9555 - loss: 0.1545  27/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9494 - loss: 0.1611 41/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9464 - loss: 0.1643 54/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9453 - loss: 0.1645 67/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9449 - loss: 0.1642 81/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9446 - loss: 0.1646 94/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9445 - loss: 0.1652107/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9444 - loss: 0.1656120/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9442 - loss: 0.1661133/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9440 - loss: 0.1665146/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9439 - loss: 0.1667159/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9438 - loss: 0.1670172/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9435 - loss: 0.1676185/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9433 - loss: 0.1682198/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9431 - loss: 0.1688211/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9428 - loss: 0.1692225/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9427 - loss: 0.1696225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9427 - loss: 0.1696 - val_accuracy: 0.4512 - val_loss: 2.6791
Epoch 12/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 19ms/step - accuracy: 0.9375 - loss: 0.1599 15/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9464 - loss: 0.1448  29/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9490 - loss: 0.1413 43/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9508 - loss: 0.1402 56/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9514 - loss: 0.1412 69/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9518 - loss: 0.1415 81/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9520 - loss: 0.1417 93/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9522 - loss: 0.1417105/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9524 - loss: 0.1415118/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9525 - loss: 0.1416131/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9524 - loss: 0.1418144/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9524 - loss: 0.1419156/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9524 - loss: 0.1420168/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9524 - loss: 0.1421181/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9523 - loss: 0.1424195/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9522 - loss: 0.1427209/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9520 - loss: 0.1432222/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9519 - loss: 0.1437225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9519 - loss: 0.1438 - val_accuracy: 0.4550 - val_loss: 2.8136
Epoch 13/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 19ms/step - accuracy: 0.9375 - loss: 0.1713 17/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9578 - loss: 0.1305  31/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9582 - loss: 0.1277 45/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9589 - loss: 0.1275 59/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9589 - loss: 0.1282 73/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9593 - loss: 0.1278 87/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9595 - loss: 0.1280100/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9595 - loss: 0.1280114/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9597 - loss: 0.1277128/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9598 - loss: 0.1276141/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9596 - loss: 0.1276154/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9595 - loss: 0.1277166/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9593 - loss: 0.1279179/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9591 - loss: 0.1280193/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9589 - loss: 0.1282206/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9586 - loss: 0.1284219/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9585 - loss: 0.1286225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9584 - loss: 0.1287 - val_accuracy: 0.4556 - val_loss: 2.9930
Epoch 14/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.8906 - loss: 0.2888 13/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9480 - loss: 0.1530  26/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9544 - loss: 0.1379 39/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9568 - loss: 0.1301 51/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9574 - loss: 0.1275 65/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9575 - loss: 0.1257 78/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9579 - loss: 0.1241 92/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9584 - loss: 0.1223105/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9587 - loss: 0.1211118/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9590 - loss: 0.1202132/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9591 - loss: 0.1199146/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9591 - loss: 0.1198160/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9592 - loss: 0.1197174/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9592 - loss: 0.1199188/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9592 - loss: 0.1201202/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9592 - loss: 0.1203216/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9591 - loss: 0.1207225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9591 - loss: 0.1209 - val_accuracy: 0.4525 - val_loss: 3.0344
Epoch 15/15
  1/225 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9688 - loss: 0.0740 16/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9705 - loss: 0.0853  31/225 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9678 - loss: 0.0905 45/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9679 - loss: 0.0914 59/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9680 - loss: 0.0926 72/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9678 - loss: 0.0939 86/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9676 - loss: 0.0951 99/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9674 - loss: 0.0961112/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9671 - loss: 0.0971124/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9669 - loss: 0.0979136/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9667 - loss: 0.0987149/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9666 - loss: 0.0993162/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9664 - loss: 0.1002175/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9662 - loss: 0.1010187/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9659 - loss: 0.1018200/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9657 - loss: 0.1025213/225 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9655 - loss: 0.1032225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9653 - loss: 0.1039 - val_accuracy: 0.4319 - val_loss: 3.1094
Accuracy (TF-IDF + Neural Network for ratings): 0.4582
  1/125 ━━━━━━━━━━━━━━━━━━━━ 5s 41ms/step 40/125 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  83/125 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step125/125 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step

Classification Report:
              precision    recall  f1-score   support

           1       0.30      0.25      0.28       138
           2       0.29      0.32      0.30       339
           3       0.39      0.35      0.37       878
           4       0.48      0.48      0.48      1441
           5       0.55      0.58      0.56      1204

    accuracy                           0.46      4000
   macro avg       0.40      0.39      0.40      4000
weighted avg       0.46      0.46      0.46      4000
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-13-output-3.png" width="885" height="758" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-13-output-4.png" width="1142" height="470" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_647939/1692797114.py:91: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x='Importance', y='Word', data=df_importance_nn, palette='viridis')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-13-output-6.png" width="949" height="1143" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="comparison-of-models-for-rating-prediction" class="level4">
<h4 class="anchored" data-anchor-id="comparison-of-models-for-rating-prediction">Comparison of Models for Rating Prediction</h4>
<p>We compared the accuracy of all three models used for rating prediction: Bag-of-Words Logistic Regression, TF-IDF Logistic Regression, and TF-IDF Neural Network. TF-IDF with Logistic Regression performed slightly better than Bag-of-Words, while the Neural Network achieved a comparable accuracy. It is worth noting that predicting the exact rating is essentially a fine-grained sentiment analysis problem. A model might easily catch that a review is generally positive but still miss whether it is 4-star or 5-star. So we expected that even our best model might only achieve moderate accuracy.</p>
<div id="83d41419" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare model accuracies for rating prediction including NN</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>models_rating_all <span class="op">=</span> [<span class="st">'BoW + LogReg'</span>, <span class="st">'TF-IDF + LogReg'</span>, <span class="st">'TF-IDF + NN'</span>]</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>accuracies_rating_all <span class="op">=</span> [accuracy_bow_rating_logistic, accuracy_tfidf_rating_logistic, accuracy_nn]</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>df_metrics_rating_all <span class="op">=</span> pd.DataFrame({<span class="st">'Model'</span>: models_rating_all, <span class="st">'Accuracy'</span>: accuracies_rating_all})</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>ax_rating_all <span class="op">=</span> sns.barplot(x<span class="op">=</span><span class="st">'Model'</span>, y<span class="op">=</span><span class="st">'Accuracy'</span>, data<span class="op">=</span>df_metrics_rating_all, palette<span class="op">=</span>[<span class="st">'#FF9671'</span>, <span class="st">'#FFC75F'</span>, <span class="st">'#845EC2'</span>])</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, acc <span class="kw">in</span> <span class="bu">enumerate</span>(accuracies_rating_all):</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    ax_rating_all.text(i, acc <span class="op">+</span> <span class="fl">0.005</span>, <span class="ss">f'</span><span class="sc">{</span>acc<span class="sc">:.4f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Accuracy Comparison for Multiclass Rating Prediction'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="fl">0.6</span>) <span class="co"># Adjust y-axis limit</span></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_647939/1326593165.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  ax_rating_all = sns.barplot(x='Model', y='Accuracy', data=df_metrics_rating_all, palette=['#FF9671', '#FFC75F', '#845EC2'])</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-14-output-2.png" width="950" height="567" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="word-clouds-for-rating-prediction" class="level3">
<h3 class="anchored" data-anchor-id="word-clouds-for-rating-prediction">Word Clouds for Rating Prediction</h3>
<p>To further explore the features learned by our models, we generated word clouds that visualize the most important words associated with each star rating (1-5) based on the coefficients from the TF-IDF Logistic Regression model.</p>
<div id="c1df136b" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate word clouds for each rating based on TF-IDF Logistic Regression model coefficients</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>vectorizer_wc <span class="op">=</span> pipeline_tfidf_rating.named_steps[<span class="st">'vectorizer'</span>]</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>classifier_wc <span class="op">=</span> pipeline_tfidf_rating.named_steps[<span class="st">'classifier'</span>]</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>feature_names_wc <span class="op">=</span> vectorizer_wc.get_feature_names_out()</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">16</span>))</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> rating_class <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>):</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">hasattr</span>(classifier_wc, <span class="st">'coef_'</span>) <span class="kw">and</span> classifier_wc.coef_.shape[<span class="dv">0</span>] <span class="op">&gt;=</span> rating_class:</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>        coefs_wc <span class="op">=</span> classifier_wc.coef_[rating_class<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>        word_importance_wc <span class="op">=</span> {feature_names_wc[i]: coefs_wc[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(feature_names_wc))}</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Filter out negative coefficients for positive word cloud</span></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>        positive_word_importance_wc <span class="op">=</span> {word: score <span class="cf">for</span> word, score <span class="kw">in</span> word_importance_wc.items() <span class="cf">if</span> score <span class="op">&gt;</span> <span class="dv">0</span>}</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>        wordcloud_rating <span class="op">=</span> WordCloud(width<span class="op">=</span><span class="dv">1000</span>, height<span class="op">=</span><span class="dv">1000</span>, background_color<span class="op">=</span><span class="st">'white'</span>, max_words<span class="op">=</span><span class="dv">150</span>, colormap<span class="op">=</span><span class="st">'viridis'</span>, prefer_horizontal<span class="op">=</span><span class="fl">1.0</span>).generate_from_frequencies(positive_word_importance_wc)</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">3</span>, rating_class)</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>        plt.imshow(wordcloud_rating, interpolation<span class="op">=</span><span class="st">'bilinear'</span>)</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="ss">f'Top Words for Rating </span><span class="sc">{</span>rating_class<span class="sc">}</span><span class="ss">'</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>plt.tight_layout(pad<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Report_files/figure-html/cell-15-output-1.png" width="1886" height="1380" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="bert-for-rating-prediction" class="level3">
<h3 class="anchored" data-anchor-id="bert-for-rating-prediction">BERT for Rating Prediction</h3>
<p>Finally, we fine-tuned DistilBERT to predict one of 5 classes. The model was trained to minimize multi-class classification loss. This model has the benefit of understanding context. For instance, it might pick up on nuanced phrases like “I really wanted to love this book” which could signal a disappointed 2 or 3-star sentiment hidden in otherwise positive wording. We hoped that the pre-trained knowledge in BERT would help it recognize intensity or tone indicators in the text that correlate with the star rating. We compared its rating prediction accuracy to the simpler models.</p>
<div id="eb1f7ce4" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust labels to be 0-indexed for categorical crossentropy in PyTorch</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>y_train_rating_adjusted <span class="op">=</span> y_train_rating <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>y_test_rating_adjusted <span class="op">=</span> y_test_rating <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataset class for BERT for rating prediction</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RatingDatasetBERT(Dataset):</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, texts, labels, tokenizer, max_len<span class="op">=</span><span class="dv">128</span>):</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.texts <span class="op">=</span> texts</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> labels</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_len <span class="op">=</span> max_len</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.texts)</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="bu">str</span>(<span class="va">self</span>.texts.iloc[idx])</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="bu">int</span>(<span class="va">self</span>.labels.iloc[idx])</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>        encoding <span class="op">=</span> <span class="va">self</span>.tokenizer(</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>            text,</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>            truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="st">'max_length'</span>,</span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>            max_length<span class="op">=</span><span class="va">self</span>.max_len,</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>            return_tensors<span class="op">=</span><span class="st">'pt'</span></span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">'input_ids'</span>: encoding[<span class="st">'input_ids'</span>].squeeze(<span class="dv">0</span>),</span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">'attention_mask'</span>: encoding[<span class="st">'attention_mask'</span>].squeeze(<span class="dv">0</span>),</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">'labels'</span>: torch.tensor(label, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Create datasets and dataloaders for rating prediction using BERT</span></span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a>train_dataset_rating_bert <span class="op">=</span> RatingDatasetBERT(X_train_rating, y_train_rating_adjusted, tokenizer_bert)</span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a>test_dataset_rating_bert <span class="op">=</span> RatingDatasetBERT(X_test_rating, y_test_rating_adjusted, tokenizer_bert)</span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a>train_loader_rating_bert <span class="op">=</span> DataLoader(train_dataset_rating_bert, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a>test_loader_rating_bert <span class="op">=</span> DataLoader(test_dataset_rating_bert, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb48-38"><a href="#cb48-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-39"><a href="#cb48-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Load DistilBERT model for sequence classification - 5 labels for ratings 1-5</span></span>
<span id="cb48-40"><a href="#cb48-40" aria-hidden="true" tabindex="-1"></a>model_rating_bert <span class="op">=</span> DistilBertForSequenceClassification.from_pretrained(<span class="st">'distilbert-base-uncased'</span>, num_labels<span class="op">=</span><span class="dv">5</span>) <span class="co"># 5 labels for ratings 1-5</span></span>
<span id="cb48-41"><a href="#cb48-41" aria-hidden="true" tabindex="-1"></a>model_rating_bert.to(device)</span>
<span id="cb48-42"><a href="#cb48-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-43"><a href="#cb48-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer and criterion</span></span>
<span id="cb48-44"><a href="#cb48-44" aria-hidden="true" tabindex="-1"></a>optimizer_rating_bert <span class="op">=</span> optim.AdamW(model_rating_bert.parameters(), lr<span class="op">=</span><span class="fl">2e-5</span>)</span>
<span id="cb48-45"><a href="#cb48-45" aria-hidden="true" tabindex="-1"></a>criterion_rating_bert <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb48-46"><a href="#cb48-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-47"><a href="#cb48-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the DistilBERT model for rating prediction</span></span>
<span id="cb48-48"><a href="#cb48-48" aria-hidden="true" tabindex="-1"></a>bert_history_rating <span class="op">=</span> train_model_bert(model_rating_bert, train_loader_rating_bert, optimizer_rating_bert, criterion_rating_bert, epochs<span class="op">=</span><span class="dv">15</span>, device<span class="op">=</span>device) <span class="co"># Increased epochs for potentially better performance</span></span>
<span id="cb48-49"><a href="#cb48-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-50"><a href="#cb48-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb48-51"><a href="#cb48-51" aria-hidden="true" tabindex="-1"></a>y_true_rating_bert, y_pred_rating_bert <span class="op">=</span> evaluate_model_bert(model_rating_bert, test_loader_rating_bert, device<span class="op">=</span>device)</span>
<span id="cb48-52"><a href="#cb48-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-53"><a href="#cb48-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert predictions back to original rating scale (1-5)</span></span>
<span id="cb48-54"><a href="#cb48-54" aria-hidden="true" tabindex="-1"></a>y_pred_rating_bert <span class="op">=</span> np.array(y_pred_rating_bert) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb48-55"><a href="#cb48-55" aria-hidden="true" tabindex="-1"></a>y_true_rating_bert <span class="op">=</span> np.array(y_true_rating_bert) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb48-56"><a href="#cb48-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-57"><a href="#cb48-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy and print classification report</span></span>
<span id="cb48-58"><a href="#cb48-58" aria-hidden="true" tabindex="-1"></a>accuracy_rating_distilbert <span class="op">=</span> accuracy_score(y_true_rating_bert, y_pred_rating_bert)</span>
<span id="cb48-59"><a href="#cb48-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy (DistilBERT for rating prediction): </span><span class="sc">{</span>accuracy_rating_distilbert<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb48-60"><a href="#cb48-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification Report (DistilBERT for rating prediction):"</span>)</span>
<span id="cb48-61"><a href="#cb48-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_true_rating_bert, y_pred_rating_bert))</span>
<span id="cb48-62"><a href="#cb48-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-63"><a href="#cb48-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion Matrix for DistilBERT Rating Prediction</span></span>
<span id="cb48-64"><a href="#cb48-64" aria-hidden="true" tabindex="-1"></a>cm_rating_bert <span class="op">=</span> confusion_matrix(y_true_rating_bert, y_pred_rating_bert)</span>
<span id="cb48-65"><a href="#cb48-65" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb48-66"><a href="#cb48-66" aria-hidden="true" tabindex="-1"></a>sns.heatmap(cm_rating_bert, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'d'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>,</span>
<span id="cb48-67"><a href="#cb48-67" aria-hidden="true" tabindex="-1"></a>            xticklabels<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>],</span>
<span id="cb48-68"><a href="#cb48-68" aria-hidden="true" tabindex="-1"></a>            yticklabels<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb48-69"><a href="#cb48-69" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted Rating'</span>)</span>
<span id="cb48-70"><a href="#cb48-70" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True Rating'</span>)</span>
<span id="cb48-71"><a href="#cb48-71" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix: TF-IDF + Neural Network for Ratings'</span>)</span>
<span id="cb48-72"><a href="#cb48-72" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb48-73"><a href="#cb48-73" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb48-74"><a href="#cb48-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-75"><a href="#cb48-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training history for BERT Rating Prediction</span></span>
<span id="cb48-76"><a href="#cb48-76" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb48-77"><a href="#cb48-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-78"><a href="#cb48-78" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot accuracy</span></span>
<span id="cb48-79"><a href="#cb48-79" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb48-80"><a href="#cb48-80" aria-hidden="true" tabindex="-1"></a>plt.plot(bert_history_rating[<span class="st">'accuracy'</span>], label<span class="op">=</span><span class="st">'Train Accuracy'</span>)</span>
<span id="cb48-81"><a href="#cb48-81" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Accuracy'</span>)</span>
<span id="cb48-82"><a href="#cb48-82" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb48-83"><a href="#cb48-83" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb48-84"><a href="#cb48-84" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb48-85"><a href="#cb48-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-86"><a href="#cb48-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot loss</span></span>
<span id="cb48-87"><a href="#cb48-87" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb48-88"><a href="#cb48-88" aria-hidden="true" tabindex="-1"></a>plt.plot(bert_history_rating[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'Train Loss'</span>)</span>
<span id="cb48-89"><a href="#cb48-89" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Loss'</span>)</span>
<span id="cb48-90"><a href="#cb48-90" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb48-91"><a href="#cb48-91" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb48-92"><a href="#cb48-92" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb48-93"><a href="#cb48-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-94"><a href="#cb48-94" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb48-95"><a href="#cb48-95" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb48-96"><a href="#cb48-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-97"><a href="#cb48-97" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare all model accuracies for rating prediction including BERT</span></span>
<span id="cb48-98"><a href="#cb48-98" aria-hidden="true" tabindex="-1"></a>models_rating_all_bert <span class="op">=</span> [<span class="st">'BoW + LogReg'</span>, <span class="st">'TF-IDF + LogReg'</span>, <span class="st">'TF-IDF + NN'</span>, <span class="st">'DistilBERT'</span>]</span>
<span id="cb48-99"><a href="#cb48-99" aria-hidden="true" tabindex="-1"></a>accuracies_rating_all_bert <span class="op">=</span> [accuracy_bow_rating_logistic, accuracy_tfidf_rating_logistic, accuracy_nn, accuracy_rating_distilbert]</span>
<span id="cb48-100"><a href="#cb48-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-101"><a href="#cb48-101" aria-hidden="true" tabindex="-1"></a>df_metrics_rating_all_bert <span class="op">=</span> pd.DataFrame({<span class="st">'Model'</span>: models_rating_all_bert, <span class="st">'Accuracy'</span>: accuracies_rating_all_bert})</span>
<span id="cb48-102"><a href="#cb48-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-103"><a href="#cb48-103" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb48-104"><a href="#cb48-104" aria-hidden="true" tabindex="-1"></a>ax_rating_all_bert <span class="op">=</span> sns.barplot(x<span class="op">=</span><span class="st">'Model'</span>, y<span class="op">=</span><span class="st">'Accuracy'</span>, data<span class="op">=</span>df_metrics_rating_all_bert, palette<span class="op">=</span>[<span class="st">'#FF9671'</span>, <span class="st">'#FFC75F'</span>, <span class="st">'#845EC2'</span>, <span class="st">'#A020F0'</span>]) <span class="co"># Added purple for BERT</span></span>
<span id="cb48-105"><a href="#cb48-105" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, acc <span class="kw">in</span> <span class="bu">enumerate</span>(accuracies_rating_all_bert):</span>
<span id="cb48-106"><a href="#cb48-106" aria-hidden="true" tabindex="-1"></a>    ax_rating_all_bert.text(i, acc <span class="op">+</span> <span class="fl">0.005</span>, <span class="ss">f'</span><span class="sc">{</span>acc<span class="sc">:.4f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb48-107"><a href="#cb48-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-108"><a href="#cb48-108" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Accuracy Comparison for Multiclass Rating Prediction (including DistilBERT)'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb48-109"><a href="#cb48-109" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb48-110"><a href="#cb48-110" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="fl">0.6</span>) <span class="co"># Adjust y-axis limit</span></span>
<span id="cb48-111"><a href="#cb48-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-112"><a href="#cb48-112" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb48-113"><a href="#cb48-113" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Here, DistilBERT achieved an accuracy of <strong>57.50%</strong>. The confusion matrix is as shown below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./Bert_confusion_matrix.png" class="img-fluid figure-img"></p>
<figcaption>Bert confusion matrix</figcaption>
</figure>
</div>
<p>DistilBERT also showed the best performance in multiclass rating prediction, achieving a higher accuracy than the simpler models. This further reinforces the effectiveness of transformer models for understanding nuanced text data like book reviews, though with increased computational cost.</p>
</section>
<section id="rating-prediction-results" class="level3">
<h3 class="anchored" data-anchor-id="rating-prediction-results">Rating Prediction Results</h3>
<p>Predicting the exact star rating out of five proved to be more challenging, whih is to be expected with more classes to choose from. None of the simple techniques managed to achieve 50% accuracy, but it was reassuring seeing the confusion matrices where the models would usually still be within one point of the true rating when it got confused. Of course, we see that DistilBERT performed superior to the other models, at a whole 8% more accuracy than the next best of TF-IDF with Logistic Regression. This improvement suggests that the context and understanding of nuances may have helped it distinguish reviews better. For example, a sentence like “I had high hopes, but it was just okay in the end” might have high coefficients for words like “high” and “hopes” while the word “okay” might not push the score down very much. A logistic model might struggle on rating this sentence.</p>
<p>Of course, we must also note that user ratings are very variable and the exact same review text could be a 5 rating by one user and a 3 rating by another user, so in that context all of the models performed remarkably well in predicting ratings based on just review text.</p>
</section>
</section>
<section id="results-analysis-and-discussion" class="level2">
<h2 class="anchored" data-anchor-id="results-analysis-and-discussion">Results, Analysis, and Discussion</h2>
<section id="summary-of-results" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-results">Summary of Results</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Task</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bag of Words + Logistic Regression</td>
<td>Sentiment Analysis</td>
<td>0.7448</td>
</tr>
<tr class="even">
<td>TF-IDF + Logistic Regression</td>
<td>Sentiment Analysis</td>
<td>0.7770</td>
</tr>
<tr class="odd">
<td>TF-IDF + Neural Networks</td>
<td>Sentiment Analysis</td>
<td>0.7410</td>
</tr>
<tr class="even">
<td>DistilBERT</td>
<td>Sentiment Analysis</td>
<td>0.8040</td>
</tr>
<tr class="odd">
<td>Bag of Words + Logistic Regression</td>
<td>Rating Prediction</td>
<td>0.4567</td>
</tr>
<tr class="even">
<td>TF-IDF + Logistic Regression</td>
<td>Rating Prediction</td>
<td>0.4935</td>
</tr>
<tr class="odd">
<td>TF-IDF + Neural Network</td>
<td>Rating Prediction</td>
<td>0.4487</td>
</tr>
<tr class="even">
<td>DistilBERT</td>
<td>Rating Prediction</td>
<td>0.5750</td>
</tr>
</tbody>
</table>
<p>To recap, in both sentiment analysis and rating prediction tasks, DistilBERT consistently outperformed the traditional machine learning models (Logistic Regression with Bag-of-Words or TF-IDF features). For sentiment analysis, DistilBERT achieved an accuracy of 80.4%, a noticeable improvement over TF-IDF Logistic Regression (77.7%) and Bag-of-Words Logistic Regression (74.5%).</p>
<p>For the more challenging task of multiclass rating prediction, all models showed lower accuracies, as expected. However, DistilBERT again led with an accuracy of 57.5%, which is significantly better than TF-IDF Logistic Regression (49.4%) and Bag-of-Words Logistic Regression (45.7%). Interestingly, the simple Neural Network did not improve upon the Logistic Regression models for rating prediction, suggesting that model complexity alone is not sufficient and that the pre-trained language understanding capabilities of DistilBERT are crucial for these tasks.</p>
</section>
</section>
<section id="impact" class="level2">
<h2 class="anchored" data-anchor-id="impact">Impact</h2>
<p>The BookGraph sentiment and rating models have several potential use cases, as well as limitations and ethical considerations.</p>
<p>A system that can automatically analyze book review text to determine sentiment or predict a likely rating could be useful in multiple ways. For readers, such models could summarize the general reception of the book. The sentiment model could estimate what percentage are positive vs negative, complementing the numerical average rating. It could also highlight a few representative highly positive or highly negative reviews by detecting sentiment extremes. For authors and publishers, sentiment analysis could provide analysis on which parts of the book readers liked or disliked, which could be inferred by looking at common words in 1-star vs 5-star reviews. Another potential impact is in moderation or recommendation systems on a platform like Goodreads. Our sentiment classifier could flag reviews that are extremely negative or extremely positive. Perhaps a platform might want to automatically detect maliciously negative reviews or spam, sentiment analysis could be one component of a larger system.</p>
<p>One positive aspect of simpler models like logistic regression is their explainability. We can directly tell a user or stakeholder, this review was marked negative because it contained words like ‘boring’ and ‘waste of time’ which are strong negative indicators. This transparency is important for trust. In contrast, a DistilBERT model would be a black box. Even if it’s more accurate, an author might not trust an AI assessment of their review without knowing why.</p>
<section id="limitations" class="level3">
<h3 class="anchored" data-anchor-id="limitations">Limitations</h3>
<p>The models inherit biases from the data. Goodreads users and their behaviors introduce bias. For instance we saw, most reviews are positive. A sentiment model might thus be biased towards predicting positive. If deployed, it might systematically undercount negative feedback. Moreover, certain genres had slightly higher ratings. The model might pick up on genre-specific language that the mode associates with higher ratings. This could be a form of genre bias i.e.&nbsp;the model could mistakenly give a higher predicted rating for a review just because it uses the style common in children’s literature reviews which typically has high review rating. If the system’s output influences any decisions (say highlighting a review or not), this could unfairly amplify some biases.</p>
<p>From an ethical standpoint, using AI to summarize opinions needs caution. Reviews are personal expressions, reducing them to “positive” or “negative” tags could lead to loss in nuance of the review. For example, marking a 3-star review as “negative” as our binary definition did, might not align with the author’s intent if they intended it as mixed or slightly positive.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This project successfully explored sentiment analysis and rating prediction for book reviews using a variety of machine learning and deep learning techniques. We found that while simpler models like Logistic Regression provide a baseline understanding, transformer-based models like DistilBERT significantly improve performance, particularly for nuanced tasks like sentiment and rating prediction from long-form text reviews. The insights gained from this project can contribute to enhancing book recommendation systems, providing valuable feedback to authors and publishers, and improving the overall reader experience on platforms like Goodreads. Further research, as discussed, can build upon these findings to develop even more accurate and impactful book review analysis tools.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Daumé III, H., &amp; Crawford, K. (2018). Datasheets for datasets. <em>Communications of the ACM</em>, <em>61</em>(12), 86-94.</p>
<p>Wan, M., &amp; McAuley, J. J. (2018, October). Item recommendation on monotonic behavior chains. In <em>Proceedings of the 12th ACM conference on recommender systems</em> (pp.&nbsp;86-94).</p>
<p>Wan, M., Misra, R., Nakashole, N., &amp; McAuley, J. (2019, July). Fine-grained spoiler detection from large-scale review corpora. In <em>Proceedings of the 57th annual meeting of the association for computational linguistics</em> (pp.&nbsp;2605-2610).</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>